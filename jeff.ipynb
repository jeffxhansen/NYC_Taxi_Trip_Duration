{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from py_files.data_manager import get_train_data, get_test_data, get_X_y\n",
    "from py_files.helper_funcs import set_np_pd_display_params\n",
    "from py_files.features import generate_features\n",
    "from config import data_path\n",
    "from shapely.wkt import loads\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "set_np_pd_display_params(np, pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'max_features': 'log2', 'min_samples_leaf': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# load the pickle models/rf_grid_search.pkl\n",
    "with open('models/rf_grid_search.pkl', 'rb') as f:\n",
    "    rf_grid_search = pickle.load(f)\n",
    "\n",
    "best_params = rf_grid_search.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter tune a RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_X_y(force_clean=True)\n",
    "feature_X = generate_features(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_X.copy()\n",
    "y_train = y.copy()\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "'''\n",
    "X_train = X_train.sample(100)\n",
    "y_train = y_train.loc[X_train.index]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'A': [1,2,3,1,2,3,1,2,3,1,2,3,4,5],\n",
    "    'B': np.arange(14),\n",
    "    'C': np.arange(14)*10\n",
    "}).sort_values(by=['A'])\n",
    "\n",
    "df_copy = df.copy()\n",
    "df = df_copy.copy()\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for _ in range(3):\n",
    "    firsts = df['A'] != df['A'].shift(1)\n",
    "    dfs.append(df.loc[firsts].copy())\n",
    "    df = df.loc[~firsts].copy()\n",
    "    \n",
    "display(df_copy)\n",
    "display(pd.concat(dfs).sort_values(by=['A']))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_X.copy()\n",
    "y_train = y.copy()\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "sample_per_class = 5\n",
    "df = X_train.copy()\n",
    "df = df.sort_values(by='avg_cluster_duration')\n",
    "display(df['avg_cluster_duration'].value_counts())\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for _ in range(4):\n",
    "    firsts = df['avg_cluster_duration'] != df['avg_cluster_duration'].shift(1)\n",
    "    dfs.append(df.loc[firsts].copy())\n",
    "    df = df.loc[~firsts].copy()\n",
    "\n",
    "final_df = pd.concat(dfs, axis=0).sort_values('avg_cluster_duration')\n",
    "X_train = final_df.copy()\n",
    "y_train = y_train.loc[X_train.index]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train.sort_values(by='avg_cluster_duration').tail(50))\n",
    "display(X_train[X_train['avg_cluster_duration'] == 845.2965812])\n",
    "display(X_train['avg_cluster_duration'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GridSearchCV object and get the optimal parameters and best score\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 400, 800, 1000],\n",
    "    'max_depth': [None, 3, 5, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 2, 3, 5],\n",
    "}\n",
    "\n",
    "\n",
    "fits = np.product([len(param) for param in param_grid.values()]) * 4\n",
    "print(fits)\n",
    "\n",
    "rf = RandomForestRegressor(warm_start=False)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, verbose=3, n_jobs=-2, cv=4).fit(X_train, y_train)\n",
    "\n",
    "with open(\"models/rf_grid_search.pkl\", \"wb\") as f:\n",
    "    pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(10*60) * (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/rf_grid_search.pkl\", \"rb\") as f:\n",
    "    grid_search = pickle.load(f)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "rf = RandomForestRegressor(**best_params, warm_start=False)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(rf.score(X_train, y_train))\n",
    "\n",
    "# print the number of leaf nodes\n",
    "print(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end hyperparameter RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test avg cluster duration feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_X_y(force_clean=True)\n",
    "feature_X = generate_features(X, y)\n",
    "display(feature_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_X['vendor_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load NYC roads as geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_X_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_df = pd.read_csv(f\"{data_path}/Centerline.csv\")\n",
    "nyc_df = nyc_df.loc[:, ['the_geom']]\n",
    "\n",
    "# Convert the \"the_geom\" column to Shapely geometries\n",
    "nyc_df['the_geom_geopandas'] = nyc_df['the_geom'].apply(loads)\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(nyc_df, geometry='the_geom_geopandas')\n",
    "\n",
    "min_lon, min_lat, max_lon, max_lat = gdf['the_geom_geopandas'].total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_str = \"\"\"\n",
    "nyc_df = pd.read_csv(f\"{data_path}/Centerline.csv\")\n",
    "nyc_df = nyc_df.loc[:, ['the_geom']]\n",
    "\n",
    "# Convert the \"the_geom\" column to Shapely geometries\n",
    "nyc_df['the_geom_geopandas'] = nyc_df['the_geom'].apply(loads)\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(nyc_df, geometry='the_geom_geopandas')\n",
    "\n",
    "min_lon, min_lat, max_lon, max_lat = gdf['the_geom_geopandas'].total_bounds\n",
    "\"\"\"\n",
    "\n",
    "with open(\"misc/lat_long_bounds.json\", \"w+\") as file:\n",
    "    lat_lon_min_max = {\n",
    "        'lat': {\n",
    "            'min': min_lat,\n",
    "            'max': max_lat\n",
    "        },\n",
    "        'lon': {\n",
    "            'min': min_lon,\n",
    "            'max': max_lon\n",
    "        },\n",
    "        'code_str': code_str\n",
    "    }\n",
    "    json.dump(lat_lon_min_max, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = X.loc[(X['pickup_longitude'] >= min_lon) & (X['pickup_longitude'] <= max_lon) &\n",
    "            (X['pickup_latitude'] >= min_lat) & (X['pickup_latitude'] <= max_lat), :].copy()\n",
    "X_small = X_small.loc[(X_small['dropoff_longitude'] >= min_lon) & (X_small['dropoff_longitude'] <= max_lon) &\n",
    "            (X_small['dropoff_latitude'] >= min_lat) & (X_small['dropoff_latitude'] <= max_lat), :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_small.shape)\n",
    "print(X.shape)\n",
    "display(X_small.head())\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the GeoDataFrame\n",
    "gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "alpha = 0.75\n",
    "\n",
    "plt.scatter(X_small['pickup_longitude'], X_small['pickup_latitude'], c='red', alpha=alpha, s=0.1, label=\"Pickup Locations\")\n",
    "leg = plt.legend(loc='upper left')\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "plt.title(\"Pickup Locations\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the GeoDataFrame\n",
    "gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "plt.scatter(X_small['dropoff_longitude'], X_small['dropoff_latitude'], c='green', alpha=alpha, s=0.1, label=\"Dropoff Locations\")\n",
    "leg = plt.legend(loc='upper left')\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "plt.title(\"Dropoff Locations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn kmeans to cluster the pickup and dropoff locations into 100 clusters\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 200\n",
    "\n",
    "kmeans_pickup = KMeans(n_clusters=n_clusters, random_state=42).fit(X_small.loc[:, ['pickup_longitude', 'pickup_latitude']].values)\n",
    "kmeans_dropoff = KMeans(n_clusters=n_clusters, random_state=42).fit(X_small.loc[:, ['dropoff_longitude', 'dropoff_latitude']].values)\n",
    "\n",
    "pickup_centers = kmeans_pickup.cluster_centers_\n",
    "dropoff_centers = kmeans_dropoff.cluster_centers_\n",
    "\n",
    "X_small['pickup_cluster'] = kmeans_pickup.predict(X_small[['pickup_longitude', 'pickup_latitude']].values)\n",
    "X_small['dropoff_cluster'] = kmeans_dropoff.predict(X_small[['dropoff_longitude', 'dropoff_latitude']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the kmeans_pickup and kmeans_dropoff in the models folder using pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"models/kmeans_200_pickup.pkl\", \"wb+\") as file:\n",
    "    pickle.dump(kmeans_pickup, file)\n",
    "    \n",
    "with open(\"models/kmeans_200_dropoff.pkl\", \"wb+\") as file:\n",
    "    pickle.dump(kmeans_dropoff, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the GeoDataFrame\n",
    "gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "plt.scatter(X_small['pickup_longitude'], X_small['pickup_latitude'], c=X_small['pickup_cluster'], cmap='magma', alpha=1.0, s=0.1, label=\"Pickup Locations\")\n",
    "plt.scatter(pickup_centers[:, 0], pickup_centers[:, 1], c='red', alpha=1, s=10, label=\"Cluster Centers\")\n",
    "leg = plt.legend(loc='upper left')\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "plt.title(\"100-KMeans Clustering for Pickup Locations\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the GeoDataFrame\n",
    "gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "plt.scatter(X_small['dropoff_longitude'], X_small['dropoff_latitude'], c=X_small['dropoff_cluster'], cmap='viridis', alpha=1.0, s=0.1, label=\"Dropoff Locations\")\n",
    "plt.scatter(dropoff_centers[:, 0], dropoff_centers[:, 1], c='blue', alpha=1, s=10, label=\"Cluster Centers\")\n",
    "leg = plt.legend(loc='upper left')\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "plt.title(\"100-KMeans Clustering for Dropoff Locations\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating the cluster distance feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_files.data_manager import get_X_y, get_nyc_gdf\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "from py_files.helper_funcs import set_np_pd_display_params\n",
    "\n",
    "set_np_pd_display_params(np, pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_X_y()\n",
    "X = generate_features(X, y)\n",
    "full_df = pd.concat([X, y], axis=1)\n",
    "nyc_gdf = get_nyc_gdf()\n",
    "display(full_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = deepcopy(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load kmeans_pickup and kmeans_dropoff from the models folder using pickle\n",
    "with open(\"models/kmeans_200_pickup.pkl\", \"rb\") as file:\n",
    "    kmeans_200_pickup = pickle.load(file)\n",
    "with open(\"models/kmeans_200_dropoff.pkl\", \"rb\") as file:\n",
    "    kmeans_200_dropoff = pickle.load(file)\n",
    "    \n",
    "# predict the clusters for the pickup and dropoff locations using the kmeans_pickup and kmeans_dropoff\n",
    "df['pickup_200_cluster'] = kmeans_200_pickup.predict(df[['pickup_longitude', 'pickup_latitude']].values)\n",
    "df['dropoff_200_cluster'] = kmeans_200_dropoff.predict(df[['dropoff_longitude', 'dropoff_latitude']].values)\n",
    "\n",
    "# get the centers\n",
    "pickup_200_centers = kmeans_200_pickup.cluster_centers_\n",
    "dropoff_200_centers = kmeans_200_dropoff.cluster_centers_\n",
    "\n",
    "# compute the average duration from cluster to cluster\n",
    "group_durations = (df\n",
    "    .groupby(['pickup_200_cluster', 'dropoff_200_cluster'])['trip_duration']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'trip_duration': 'avg_cluster_duration'}))\n",
    "\n",
    "# merge the average duration from cluster to cluster with the main dataframe\n",
    "df = pd.merge(\n",
    "    left=df, right=group_durations, how='left',\n",
    "    left_on=['pickup_200_cluster', 'dropoff_200_cluster'], right_on=['pickup_200_cluster', 'dropoff_200_cluster'])\n",
    "\n",
    "# fill the missing values with the mean of the average duration from cluster to cluster\n",
    "df['avg_cluster_duration'] = df['avg_cluster_duration'].fillna(df['avg_cluster_duration'].mean())\n",
    "#df.drop(columns=['pickup_200_cluster', 'dropoff_200_cluster'], inplace=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_cluster_df = pd.DataFrame({\n",
    "    'pickup_200_cluster': np.arange(0, 200),\n",
    "    'pickup_longitude': pickup_200_centers[:, 0],\n",
    "    'pickup_latitude': pickup_200_centers[:, 1]\n",
    "})\n",
    "dropoff_cluster_df = pd.DataFrame({\n",
    "    'dropoff_200_cluster': np.arange(0, 200),\n",
    "    'dropoff_longitude': dropoff_200_centers[:, 0],\n",
    "    'dropoff_latitude': dropoff_200_centers[:, 1]\n",
    "})\n",
    "\n",
    "display(pickup_cluster_df.head())\n",
    "display(dropoff_cluster_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_durations_sorted = group_durations.sort_values(by='avg_cluster_duration', ascending=False)\n",
    "n = 1000\n",
    "top_n = group_durations_sorted.iloc[-n:, :]\n",
    "\n",
    "top_n = pd.merge(\n",
    "    left=top_n, right=pickup_cluster_df, how='left',\n",
    "    left_on=['pickup_200_cluster'], right_on=['pickup_200_cluster'])\n",
    "top_n = pd.merge(\n",
    "    left=top_n, right=dropoff_cluster_df, how='left',\n",
    "    left_on=['dropoff_200_cluster'], right_on=['dropoff_200_cluster'])\n",
    "\n",
    "pickup_longitudes = top_n['pickup_longitude'].values\n",
    "pickup_latitudes = top_n['pickup_latitude'].values\n",
    "dropoff_longitudes = top_n['dropoff_longitude'].values\n",
    "dropoff_latitudes = top_n['dropoff_latitude'].values\n",
    "\n",
    "# plot the GeoDataFrame\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(12, 12)\n",
    "\n",
    "# plot the GeoDataFrame on the current ax\n",
    "nyc_gdf.plot(ax=ax, linewidth=0.1, edgecolor='black', alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "#nyc_gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "plt.scatter(pickup_longitudes, pickup_latitudes, c='blue', alpha=1.0, s=10, label=\"Pickup Locations\")\n",
    "plt.scatter(dropoff_longitudes, dropoff_latitudes, c='yellow', alpha=1.0, s=10, label=\"Dropoff Locations\")\n",
    "    \n",
    "for plong, plat, dlong, dlat in zip(pickup_longitudes, pickup_latitudes, dropoff_longitudes, dropoff_latitudes):\n",
    "    x = np.linspace(plong, dlong, 100)\n",
    "    y = np.linspace(plat, dlat, 100)\n",
    "    cols = np.linspace(0, 1, 100)\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "                              \n",
    "    lc = LineCollection(segments, cmap='viridis')\n",
    "    lc.set_array(cols)\n",
    "    lc.set_linewidth(2)\n",
    "    lc.set_alpha(0.5)\n",
    "    ax.add_collection(lc)\n",
    "    \n",
    "    #plt.plot([plong, dlong], [plat, dlat], color='black', alpha=0.2)\n",
    "    \n",
    "plt.title(f\"{n} Clusters with shortest average trip duration\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_durations_sorted = group_durations.sort_values(by='avg_cluster_duration', ascending=True)\n",
    "cluster_dist = group_durations_sorted.copy()\n",
    "\n",
    "cluster_dist = pd.merge(\n",
    "    left=cluster_dist, right=pickup_cluster_df, how='left',\n",
    "    left_on=['pickup_200_cluster'], right_on=['pickup_200_cluster'])\n",
    "cluster_dist = pd.merge(\n",
    "    left=cluster_dist, right=dropoff_cluster_df, how='left',\n",
    "    left_on=['dropoff_200_cluster'], right_on=['dropoff_200_cluster'])\n",
    "\n",
    "pickup_longitudes = cluster_dist['pickup_longitude'].values\n",
    "pickup_latitudes = cluster_dist['pickup_latitude'].values\n",
    "dropoff_longitudes = cluster_dist['dropoff_longitude'].values\n",
    "dropoff_latitudes = cluster_dist['dropoff_latitude'].values\n",
    "\n",
    "cluster_dist['distance'] = np.sqrt((pickup_longitudes - dropoff_longitudes)**2 + (pickup_latitudes - dropoff_latitudes)**2)\n",
    "cluster_dist['duration_dist_ratio'] = cluster_dist['avg_cluster_duration'] / cluster_dist['distance']\n",
    "cluster_dist = cluster_dist.sort_values(by='duration_dist_ratio', ascending=False)\n",
    "display(cluster_dist)\n",
    "\n",
    "n = 1000\n",
    "worst_traffic = cluster_dist.iloc[:n, :]\n",
    "\n",
    "pickup_longitudes = worst_traffic['pickup_longitude'].values\n",
    "pickup_latitudes = worst_traffic['pickup_latitude'].values\n",
    "dropoff_longitudes = worst_traffic['dropoff_longitude'].values\n",
    "dropoff_latitudes = worst_traffic['dropoff_latitude'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# plot the GeoDataFrame\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(12, 12)\n",
    "\n",
    "# plot the GeoDataFrame on the current ax\n",
    "nyc_gdf.plot(ax=ax, linewidth=0.1, edgecolor='black', alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "#nyc_gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "plt.scatter(pickup_longitudes, pickup_latitudes, c='blue', alpha=1.0, s=10, label=\"Pickup Locations\")\n",
    "plt.scatter(dropoff_longitudes, dropoff_latitudes, c='red', alpha=1.0, s=10, label=\"Dropoff Locations\")\n",
    "    \n",
    "for plong, plat, dlong, dlat in zip(pickup_longitudes, pickup_latitudes, dropoff_longitudes, dropoff_latitudes):\n",
    "    x = np.linspace(plong, dlong, 100)\n",
    "    y = np.linspace(plat, dlat, 100)\n",
    "    cols = np.linspace(0, 1, 100)\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "                              \n",
    "    lc = LineCollection(segments, cmap='coolwarm')\n",
    "    lc.set_array(cols)\n",
    "    lc.set_linewidth(2)\n",
    "    #lc.set_alpha(0.2)\n",
    "    ax.add_collection(lc)\n",
    "    \n",
    "    #plt.plot([plong, dlong], [plat, dlat], color='black', alpha=0.2)\n",
    "plt.title(f\"{n} Clusters with longest average trip duration per distance\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_traffic = cluster_dist.iloc[-n:, :]\n",
    "\n",
    "pickup_longitudes = best_traffic['pickup_longitude'].values\n",
    "pickup_latitudes = best_traffic['pickup_latitude'].values\n",
    "dropoff_longitudes = best_traffic['dropoff_longitude'].values\n",
    "dropoff_latitudes = best_traffic['dropoff_latitude'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot the GeoDataFrame\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(12, 12)\n",
    "\n",
    "# plot the GeoDataFrame on the current ax\n",
    "nyc_gdf.plot(ax=ax, linewidth=0.1, edgecolor='black', alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "#nyc_gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "plt.scatter(pickup_longitudes, pickup_latitudes, c='blue', alpha=1.0, s=10, label=\"Pickup Locations\")\n",
    "plt.scatter(dropoff_longitudes, dropoff_latitudes, c='red', alpha=1.0, s=10, label=\"Dropoff Locations\")\n",
    "    \n",
    "for plong, plat, dlong, dlat in zip(pickup_longitudes, pickup_latitudes, dropoff_longitudes, dropoff_latitudes):\n",
    "    x = np.linspace(plong, dlong, 100)\n",
    "    y = np.linspace(plat, dlat, 100)\n",
    "    cols = np.linspace(0, 1, 100)\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "                              \n",
    "    lc = LineCollection(segments, cmap='coolwarm')\n",
    "    lc.set_array(cols)\n",
    "    #lc.set_linewidth(50)\n",
    "    #lc.set_alpha(0.2)\n",
    "    ax.add_collection(lc)\n",
    "    \n",
    "    #plt.plot([plong, dlong], [plat, dlat], color='black', alpha=0.2)\n",
    "plt.title(f\"{n} Clusters with shortest average trip duration per distance\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "x    = np.linspace(0,1, 100)\n",
    "y    = np.linspace(0,1, 100)\n",
    "cols = np.linspace(0,1,len(x))\n",
    "\n",
    "points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "fig, ax = plt.subplots()\n",
    "lc = LineCollection(segments, cmap='coolwarm')\n",
    "lc.set_array(cols)\n",
    "lc.set_linewidth(2)\n",
    "line = ax.add_collection(lc)\n",
    "fig.colorbar(line,ax=ax)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plongs = np.random.random(10)\n",
    "plats = np.random.random(10)\n",
    "dlongs = np.random.random(10)\n",
    "dlats = np.random.random(10)\n",
    "\n",
    "plt.scatter(plongs, plats, c='blue', alpha=1.0, s=10, label=\"Pickup Locations\")\n",
    "plt.scatter(dlongs, dlats, c='red', alpha=1.0, s=10, label=\"Dropoff Locations\")\n",
    "\n",
    "for plong, plat, dlong, dlat in zip(plongs, plats, dlongs, dlats):\n",
    "    x = np.linspace(plong, dlong, 100)\n",
    "    y = np.linspace(plat, dlat, 100)\n",
    "    cols = np.linspace(0, 1, 100)\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "                              \n",
    "    lc = LineCollection(segments, cmap='coolwarm')\n",
    "    lc.set_array(cols)\n",
    "    lc.set_linewidth(2)\n",
    "    ax.add_collection(lc)\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(group_durations.sort_values(by='avg_cluster_duration', ascending=True))\n",
    "\n",
    "# histograp of group_durations['avg_cluster_duration']\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(group_durations['avg_cluster_duration'], bins=100)\n",
    "plt.xlabel(\"Average Duration (seconds)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Average Duration from Cluster to Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a heatmap of group_durations where X-axis is pickup_200_cluster and Y-axis is dropoff_200_cluster\n",
    "# and the heat values is the average duration from cluster to cluster\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pivot_df = group_durations.pivot(index='pickup_200_cluster', columns='dropoff_200_cluster', values='avg_cluster_duration')\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(pivot_df, cmap='viridis')\n",
    "plt.title(\"Average Duration from Cluster to Cluster\")\n",
    "plt.xlabel(\"Dropoff Cluster\")\n",
    "plt.ylabel(\"Pickup Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = pd.Series(np.array([1,2,np.nan,3,np.nan]))\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "test = pd.DataFrame({\n",
    "    'A':[1,2,1,2,1,2,1,2,1,2,1,2],\n",
    "    'B':[1,2,3,1,2,3,1,2,3,1,2,3],\n",
    "    'C': np.random.random(12),\n",
    "    'D': np.arange(12)\n",
    "})\n",
    "display(test)\n",
    "\n",
    "thingy = (test.groupby(['A', 'B'])['C']\n",
    "          .mean()\n",
    "          .reset_index()\n",
    "          .rename(columns={'C': 'mean'}))\n",
    "display(thingy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.598658+0.708073)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end load NYC roads as geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): return -1.7*x**3 + 4.2*x**2 + 3.1*x + 2.3\n",
    "\n",
    "n = 100\n",
    "xs = np.linspace(-10, 10, n)\n",
    "xs_noise = np.linspace(-10, 10, n) + np.random.normal(0, 1, n)\n",
    "ys = f(xs)\n",
    "ys_noise = f(xs_noise)\n",
    "ys_noise += np.random.normal(0, 300, n)\n",
    "\n",
    "plt.scatter(xs_noise, ys_noise)\n",
    "#plt.plot(xs, ys, color='red')\n",
    "\n",
    "X = np.hstack([xs_noise.reshape((-1,1))**j for j in range(0, 4)])\n",
    "\n",
    "b_hat = np.linalg.inv(X.T @ X) @ X.T @ ys_noise\n",
    "def approx_f(x, b_hat): return b_hat[0] + b_hat[1]*x + b_hat[2]*x**2 + b_hat[3]*x**3\n",
    "\n",
    "sortation = np.argsort(xs_noise)\n",
    "xs_noise = xs_noise[sortation]\n",
    "ys_noise = ys_noise[sortation]\n",
    "approx_y = approx_f(xs_noise, b_hat)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(xs_noise, approx_f(xs_noise, b_hat), color='green')\n",
    "plt.show()\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerline_df = pd.read_csv(f\"{data_path}/Centerline.csv\")\n",
    "display(centerline_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_train_data()\n",
    "test = get_test_data()\n",
    "X, y = get_X_y()\n",
    "\n",
    "display(train)\n",
    "display(test)\n",
    "display(X)\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X['vendor_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figuring out data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from config import data_path\n",
    "\n",
    "train = pd.read_csv(f'{data_path}/train.csv')\n",
    "test = pd.read_csv(f'{data_path}/test.csv')\n",
    "\n",
    "cols_to_drop = ['id', 'store_and_fwd_flag', 'dropoff_datetime']\n",
    "\n",
    "train_cols_to_drop = [c for c in train.columns if c in cols_to_drop]\n",
    "train_cleaned = train.drop(columns=train_cols_to_drop)\n",
    "\n",
    "test_cols_to_drop = [c for c in test.columns if c in cols_to_drop]\n",
    "test_cleaned = test.drop(columns=test_cols_to_drop)\n",
    "\n",
    "\n",
    "display(train_cleaned)\n",
    "display(test_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying nyc on XGBoost vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/jeffx/ACME/senior/vol3/_Homework/data/nyc-taxi-trip-duration/train.csv\"\n",
    "nyc_data = pd.read_csv(data_path)\n",
    "rel_cols = [\n",
    "    'pickup_datetime', 'dropoff_datetime', 'passenger_count', \n",
    "    'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \n",
    "    'dropoff_latitude', 'trip_duration']\n",
    "data = nyc_data[rel_cols].copy()\n",
    "data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime']).astype('int64') // 10**9\n",
    "data['dropoff_datetime'] = pd.to_datetime(data['dropoff_datetime']).astype('int64') // 10**9\n",
    "# scale the pickup and dropoff datetimes with min-max scaling\n",
    "data['pickup_datetime'] = (data['pickup_datetime'] - data['pickup_datetime'].min()) / (data['pickup_datetime'].max() - data['pickup_datetime'].min())\n",
    "data['dropoff_datetime'] = (data['dropoff_datetime'] - data['dropoff_datetime'].min()) / (data['dropoff_datetime'].max() - data['dropoff_datetime'].min())\n",
    "\n",
    "X = data.drop('trip_duration', axis=1).copy().values\n",
    "y = data['trip_duration'].copy().values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_regres = xgb.XGBRegressor()\n",
    "xgb_regres.fit(X_train, y_train)\n",
    "y_pred = xgb_regres.predict(X_test)\n",
    "y_pred = y_pred.astype(int)\n",
    "y_pred[y_pred < 0] = 0\n",
    "\n",
    "print(y_pred[y_pred < 0])\n",
    "\n",
    "print(np.sum(y_pred >= 0), len(y_pred))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "# compute the Root Mean Squared Logarithmic Error\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y) - np.log1p(y_pred))))\n",
    "print(\"RMSLE: \", rmsle(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict type of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 5.18</h1>\n",
    "\n",
    "Apply a random forest regressor or classifier to your final project dataset.\n",
    "\n",
    "(i) Optimize the choice of hyperparameters, including <code>n_estimators</code>, <code>max_depth</code> and <code>max_features</code>.\n",
    "\n",
    "(ii) Compare the performance of the random forest to the performance of other types of models on this dataset.\n",
    "\n",
    "(iii) If appropriate for this dataset, identify the three most and least important features. If this is not appropriate to do for this dataset, explain why it is not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import fetch_california_housing, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seaborn import load_dataset as sns_load_dataset\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import xgboost as xgb\n",
    "\n",
    "data_path = \"/home/jeffx/ACME/WearableHealthPredictor/data\"\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "pd.set_option(\"display.precision\", 15)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.7f}\".format(x)})\n",
    "pd.set_option('display.float_format', lambda x: \"{0:0.7f}\".format(x))\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def movecol(df, cols_to_move=[], ref_col='', place='After'):\n",
    "    \"\"\"Moves columns around in a dtaframe\n",
    "    Examples:\n",
    "    1)\n",
    "        df cols ['A', 'B', 'C', 'D', 'X', 'Y', 'Z']\n",
    "        movecol(df, cols_to_move=['X', 'Y', 'Z'], ref_col='B', place='After')\n",
    "        df cols ['A', 'B', 'X', 'Y', 'Z', 'C', 'D']]\n",
    "        \n",
    "    2)\n",
    "        df cols ['A', 'B', 'C', 'D', 'X', 'Y', 'Z']\n",
    "        movecol(df, cols_to_move=['X', 'Y', 'Z'], ref_col='A', place='Before')\n",
    "        df cols ['X', 'Y', 'Z', 'A', 'B', 'C', 'D']\n",
    "    \"\"\"\n",
    "    cols = df.columns.tolist()\n",
    "    if place == 'After':\n",
    "        seg1 = cols[:list(cols).index(ref_col) + 1]\n",
    "        seg2 = cols_to_move\n",
    "    if place == 'Before':\n",
    "        seg1 = cols[:list(cols).index(ref_col)]\n",
    "        seg2 = cols_to_move + [ref_col]\n",
    "    seg1 = [i for i in seg1 if i not in seg2]\n",
    "    seg3 = [i for i in cols if i not in seg1 + seg2]\n",
    "    return(df[seg1 + seg2 + seg3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the csv and select only the relevant columns\n",
    "df_exercise = pd.read_csv(f\"{data_path}/activity_environment_data.csv\")\n",
    "rel_cols = [\n",
    "    'Steps', 'Calories_Burned', 'Distance_Covered', \n",
    "    'Exercise_Duration', 'Exercise_Intensity', \n",
    "    'Ambient_Temperature', 'Altitude', 'UV_Exposure'] + ['Exercise_Type']\n",
    "df_exercise = df_exercise[rel_cols]\n",
    "\n",
    "# convert exercise intensity to a numeric value\n",
    "df_exercise['Exercise_Intensity'] = df_exercise['Exercise_Intensity'].fillna('None')\n",
    "df_exercise['Exercise_Intensity'] = df_exercise['Exercise_Intensity'].map(\n",
    "    {'None': 0, 'Low':1, 'Moderate':2, 'High':3}\n",
    ")\n",
    "display(df_exercise)\n",
    "\n",
    "for exercise_type, frame in df_exercise.groupby('Exercise_Type'):\n",
    "    print(exercise_type)\n",
    "    display(frame.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your data (replace 'your_data.csv' with the actual path to your data)\n",
    "data = df_exercise\n",
    "\n",
    "# Select the features you want to use for clustering\n",
    "features = data[['Steps', 'Calories_Burned', 'Distance_Covered', 'Exercise_Duration',\n",
    "                 'Exercise_Intensity', 'Ambient_Temperature', 'Altitude', 'UV_Exposure']].copy().values\n",
    "\n",
    "# Standardize the data (important for K-Means)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Create a K-Means model with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit the model to your data\n",
    "kmeans.fit(scaled_features)\n",
    "\n",
    "# Add cluster labels to your DataFrame\n",
    "data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# View the results, e.g., the count of data points in each cluster\n",
    "cluster_counts = data['Cluster'].value_counts()\n",
    "print(cluster_counts)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(scaled_features)\n",
    "print(pca_data.shape)\n",
    "\n",
    "# You can also plot the clusters if you want to visualize the results\n",
    "plt.scatter(pca_data[:,0], pca_data[:,1], c=kmeans.labels_, cmap='viridis', alpha=0.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in each of the dataframes\n",
    "data_path = \"/home/jeffx/ACME/WearableHealthPredictor/data\"\n",
    "df = pd.read_csv(f\"{data_path}/health_data_clean.csv\")\n",
    "\n",
    "# merge the Exercise_Type columns together to get it as a predicted column\n",
    "all_exercise_type_cols = [\"Exercise_Type_None\",\t\"Exercise_Type_Running\", \"Exercise_Type_Strength Training\", \"Exercise_Type_Yoga\"]\n",
    "exercise_type_cols = [\"Exercise_Type_Running\", \"Exercise_Type_Strength Training\", \"Exercise_Type_Yoga\"]\n",
    "for i, col in enumerate(exercise_type_cols):\n",
    "    df[col] = df[col] * (i + 1)\n",
    "df[\"Exercise_Type\"] = df[exercise_type_cols].sum(axis=1) - 1\n",
    "df = df.drop(columns=all_exercise_type_cols)\n",
    "df = df.loc[df['Exercise_Type'] != -1, :]\n",
    "\n",
    "# only consider the relevent columns\n",
    "rel_cols = [\n",
    "    'Steps', 'Calories_Burned', 'Distance_Covered', \n",
    "    'Exercise_Duration', 'Exercise_Intensity', \n",
    "    'Ambient_Temperature', 'Altitude', 'UV_Exposure']\n",
    "\n",
    "df = df[rel_cols + [\"Exercise_Type\"]]\n",
    "\n",
    "# split the data into X and y for train and test\n",
    "X_df = df.drop(columns=[\"Exercise_Type\"])\n",
    "features = X_df.columns.tolist()\n",
    "X = X_df.copy().values\n",
    "\n",
    "y = df[\"Exercise_Type\"].copy().values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n",
    "\n",
    "for exercise_type, frame in df.groupby(\"Exercise_Type\"):\n",
    "    print(exercise_type)\n",
    "    display(frame.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train optimal RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [None, 3, 5, 10, 20],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object and get the optimal parameters and best score\n",
    "rf = RandomForestClassifier(warm_start=False)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "# get the best params and accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "# save the best_params and best_accuracy so I don't have to rerun the GridSearchCV again\n",
    "import json\n",
    "with open(\"best_params.json\", \"w\") as f:\n",
    "    obj = {\n",
    "        'best_accuracy': float(round(best_accuracy, 5)), \n",
    "        'best_params':best_params\n",
    "    }\n",
    "    json.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best params and accuracy\n",
    "with open(\"best_params.json\") as file:\n",
    "    obj = json.load(file)\n",
    "    best_accuracy = obj['best_accuracy']\n",
    "    best_params = obj['best_params']\n",
    "\n",
    "# train a new model using the optimal parameters\n",
    "rf_optimal = RandomForestClassifier(**best_params, warm_start=False)\n",
    "rf_optimal.fit(X_train, y_train)\n",
    "rf_predictions = rf_optimal.predict(X_test)\n",
    "rf_accuracy = rf_optimal.score(X_test, y_test)\n",
    "\n",
    "# select the top 5 features\n",
    "feature_importance = rf_optimal.feature_importances_\n",
    "features_sorted = sorted(list(zip(features, feature_importance)), key=lambda x: x[1], reverse=True)\n",
    "top_5_features = features_sorted[:5]\n",
    "top_features, top_feature_scores = map(list, zip(*top_5_features))\n",
    "\n",
    "# print out the best parameters and the Accuracy\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "print(\"RandomForest Accuracy:\", rf_accuracy)\n",
    "print(\"Top 5 Features:\", top_features)\n",
    "print(\"Top 5 Feature Scores:\", top_feature_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train an XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and predict with a default XGBoost model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# get the accuracy and feature importance\n",
    "xgb_accuracy = xgb_model.score(X_test, y_test)\n",
    "xgb_feature_importance = xgb_model.feature_importances_\n",
    "xgb_features_sorted = sorted(list(zip(features, xgb_feature_importance)), key=lambda x: x[1], reverse=True)\n",
    "xgb_top_5_features = xgb_features_sorted[:5]\n",
    "xgb_top_features, xgb_top_feature_scores = map(list, zip(*xgb_top_5_features))\n",
    "\n",
    "# print out the accuracy and top 5 features\n",
    "print(\"XGB Accuracy:\", xgb_accuracy)\n",
    "print(\"XGB Top 5 Features:\", xgb_top_features)\n",
    "print(\"XGB Top 5 Feature Scores:\", xgb_top_feature_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and predict with a default DecisionTree model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_predictions = dt_model.predict(X_test)\n",
    "\n",
    "# get the accuracy and feature importance\n",
    "dt_accuracy = dt_model.score(X_test, y_test)\n",
    "dt_feature_importance = dt_model.feature_importances_\n",
    "dt_features_sorted = sorted(list(zip(features, dt_feature_importance)), key=lambda x: x[1], reverse=True)\n",
    "dt_top_5_features = dt_features_sorted[:5]\n",
    "dt_top_features, dt_top_feature_scores = map(list, zip(*dt_top_5_features))\n",
    "\n",
    "# print out the accuracy and top 5 features\n",
    "print(\"DT Accuracy:\", dt_accuracy)\n",
    "print(\"DT Top 5 Features:\", dt_top_features)\n",
    "print(\"DT Top 5 Feature Scores:\", dt_top_feature_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display an accuracy table for the 3 models\n",
    "accuracy_df = pd.DataFrame({\n",
    "    'Model': ['RandomForest', 'XGBoost', 'DecisionTree'],\n",
    "    'Accuracy': [rf_accuracy, xgb_accuracy, dt_accuracy],\n",
    "    'Optimized': [True, False, False]\n",
    "})\n",
    "display(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each of the ROC curves\n",
    "def plot_roc_auc(fpr, tpr, roc_auc, classifier, data_name):\n",
    "    \"\"\"takes in the false-positive-rate, the true-positive-rate and the\n",
    "    area-under-the-curve, the name of the classifier, and the name of the dataset\n",
    "    and plots the ROC curver with the AUC\n",
    "    \"\"\"\n",
    "    plt.plot(fpr, tpr, color='yellow', lw=5, label=f\"{classifier} for {data_name}\")\n",
    "    plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve, AUC={round(roc_auc, 3)}')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "# for each model plot the ROC curve for each class\n",
    "models = [('RandomForest', rf_optimal), ('XGBoost', xgb_model), ('DecisionTree', dt_model)]\n",
    "classes = [\"Running\", \"Strength Training\", \"Yoga\"]\n",
    "\n",
    "for i, (model_name, model) in enumerate(models):\n",
    "    \n",
    "    probs = model.predict_proba(X_test)\n",
    "    print(probs)\n",
    "    \n",
    "    # for each class\n",
    "    for j, probs_col in enumerate(probs.T):\n",
    "        curr_class = classes[j]\n",
    "        \n",
    "        # select only the rows that are the current class\n",
    "        mask = y_test == j\n",
    "        y_test_curr_class = np.zeros(len(y_test))\n",
    "        y_test_curr_class[mask] = 1\n",
    "        \n",
    "        # get the ROC curve and the AUC (Area Under the Curve) values\n",
    "        fpr, tpr, thresholds = roc_curve(y_test_curr_class, probs_col)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.subplot(3, 4, i*4 + j+1)\n",
    "        plot_roc_auc(fpr, tpr, roc_auc, model_name, curr_class)\n",
    "    \n",
    "# show the final plot\n",
    "plt.gcf().set_size_inches(20,15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
