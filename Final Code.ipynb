{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:** Jeff Hansen, Dylan Skinner, Jason Vasquez, Dallin Stewart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python imports\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import seaborn as sbn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "# Native imports\n",
    "from py_files.features import generate_features\n",
    "from py_files.data_manager import get_X_y, get_nyc_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_feature_selection(X, y):\n",
    "    \"\"\"\n",
    "    Performs feature selection using the LassoLarsIC method\n",
    "\n",
    "    Parameters\n",
    "    - X (dataframe): dataframe of input features\n",
    "    - y (series): series of target values\n",
    "\n",
    "    Returns:\n",
    "    - (dict): dictionary of results (optimal alpha, optimal BIC, lasso coefficients, important features)\n",
    "    \"\"\"\n",
    "    lasso_lars_ic = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC(criterion=\"bic\", normalize=False)).fit(X, y)\n",
    "\n",
    "    results = pd.DataFrame(\n",
    "        {\n",
    "            \"alphas\": lasso_lars_ic[-1].alphas_,\n",
    "            \"BIC criterion\": lasso_lars_ic[-1].criterion_,\n",
    "        }\n",
    "    ).set_index(\"alphas\")\n",
    "\n",
    "    optimal_alpha = results[results['BIC criterion'] == results['BIC criterion'].min()].index\n",
    "\n",
    "    # Train a Lasso model with the optimal alpha for feature selection\n",
    "    lasso = linear_model.Lasso(alpha=optimal_alpha)\n",
    "    lasso.fit(X, y)\n",
    "\n",
    "    return {'Optimal Alpha': optimal_alpha.values[0], 'Optimal BIC': results.loc[optimal_alpha].values[0].tolist()[0],\n",
    "            'Lasso Coeffs': lasso.coef_.round(4), 'Important Features': X.columns[lasso.coef_ != 0].values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression_model(optimal_alpha):\n",
    "    \"\"\"\n",
    "    This function takes in the optimal alpha from the Lasso Lars IC Feature Selection and trains a Lasso Regression\n",
    "    model on the important features.\n",
    "\n",
    "    Parametes:\n",
    "    - optimal_alpha (float): The optimal alpha from the Lasso Lars IC Feature Selection\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with the RMSE of the model\n",
    "    \"\"\"\n",
    "    # Important features selected from Lasso Lars IC Feature Selection\n",
    "    important_features = ['pickup_minute', 'distance_km', 'temperature_2m (Â°C)', 'cloudcover (%)', 'avg_cluster_duration']\n",
    "\n",
    "    # Create dataframe of important features\n",
    "    X2 = X[important_features]\n",
    "\n",
    "    # Get test train split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and fit the model.\n",
    "    model = linear_model.Lasso(alpha=optimal_alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test data and compute RMSE\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {'RMSE': mean_squared_error(y_test, y_pred, squared=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_hyperparameter_search(X, y):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter search for LightGBM model\n",
    "\n",
    "    Parameters:\n",
    "    - X (dataframe): dataframe of input features\n",
    "    - y (dataframe): dataframe of target variables\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary of best parameters and best RMSE\n",
    "    \"\"\"\n",
    "    # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Create param grid\n",
    "    param_grid = {\n",
    "        'boosting_type': ['gbdt', 'dart'],\n",
    "        'num_leaves': [30, 40],\n",
    "        'learning_rate': [0.01, 0.05],\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20],\n",
    "        'reg_alpha': [0.1, 0.5],\n",
    "        'reg_lambda': [0.1, 0.5],\n",
    "    }\n",
    "\n",
    "    # LightGBM\n",
    "    lgb_train = lgb.LGBMRegressor()\n",
    "\n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(estimator=lgb_train, param_grid=param_grid, cv=3, scoring='neg_root_mean_squared_error', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Validate\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    return {'Best parameters from grid search': grid_search.best_params_, 'Best RMSE': mean_squared_error(y_test, y_pred, squared=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_hyperparameter_search(X, y):\n",
    "    \"\"\"\n",
    "    Performs a grid search on the XGBoost model\n",
    "    \n",
    "    Parameters:\n",
    "    - X (DataFrame): The input features\n",
    "    - y (Series): The target variable\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    param_grid = {\n",
    "        'booster': ['gbtree', 'dart'],\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.05],\n",
    "        'max_depth': [10, 20],\n",
    "        'alpha': [0.1, 0.5],\n",
    "    }\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_root_mean_squared_error', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best params\n",
    "    print('Best parameters from grid search: ', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_model(X, y):\n",
    "    \"\"\"\n",
    "    XGBoost model with optimal hyperparameters\n",
    "    \n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary of RMSE results\n",
    "    \"\"\"\n",
    "    # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBRegressor(booster='gbtree', n_estimators=100, learning_rate=0.01, max_depth=10, alpha=0.1)\n",
    "\n",
    "    # Fit\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validate\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    return {'RMSE': mean_squared_error(y_test, y_pred, squared=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_gridsearch():\n",
    "    \"\"\"\n",
    "    Performs a hyperparameter gridsearch with cross validation\n",
    "    to find the optimal parameters for a RandomForestRegressor\n",
    "    \n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "        \n",
    "    # get the X and y, and add the features\n",
    "    X, y = get_X_y(force_clean=True)\n",
    "    feature_X = generate_features(X, y)\n",
    "\n",
    "    # drop the pickup datetime feature since sklearn RandomForest does\n",
    "    # not accept datetime columns\n",
    "    feature_X = feature_X.drop(columns=['pickup_datetime'])\n",
    "\n",
    "    # get the X and y for training\n",
    "    X_train = feature_X.copy()\n",
    "    y_train = y.copy()\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "    # to speed up the grid search, we will use the first four instances\n",
    "    # of each cluster-to-cluster pair of data points\n",
    "    df = X_train.copy()\n",
    "    df = df.sort_values(by='avg_cluster_duration')\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    sample_per_class = 4\n",
    "    for _ in range(sample_per_class):\n",
    "        firsts = df['avg_cluster_duration'] != df['avg_cluster_duration'].shift(1)\n",
    "        dfs.append(df.loc[firsts].copy())\n",
    "        df = df.loc[~firsts].copy()\n",
    "\n",
    "    # combine all of the reprsentative samples\n",
    "    final_df = pd.concat(dfs, axis=0).sort_values('avg_cluster_duration')\n",
    "\n",
    "    # shuffle the data so that it is no longer sorted by avg_cluster_duration\n",
    "    X_train = final_df.copy().sample(frac=1)\n",
    "    y_train = y_train.loc[X_train.index]\n",
    "\n",
    "    X_train = X_train.values.astype(np.float32)\n",
    "    y_train = y_train.values.astype(np.float32)\n",
    "\n",
    "    # perform the RandomForest gridsearch to find the best\n",
    "    # hyperparameters\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 400, 800, 1000],\n",
    "        'max_depth': [None, 3, 5, 10, 20],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'min_samples_leaf': [1, 2, 3, 5],\n",
    "    }\n",
    "    rf = RandomForestRegressor(warm_start=False)\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, verbose=3, n_jobs=-2, cv=4).fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # save the grid_search_model for future loading\n",
    "    with open(\"models/rf_grid_search.pkl\", \"wb\") as f:\n",
    "        pickle.dump(grid_search, f)\n",
    "        \n",
    "    # print the best parameters\n",
    "    print(\"Best parameters RandomForest:\", best_params)\n",
    "    \n",
    "    # train a model and compute the RMSE on the test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    rf = RandomForestRegressor(**best_params)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    rf_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    print(\"RandomForest RMSE:\", rf_rmse)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_pickup_dropoff_model(df, n_clusters=200):\n",
    "    \"\"\"\n",
    "    Fits KMeans models for pickup and dropoff locations, labels each location by its cluster,\n",
    "    computes the average duration between each cluster, and merges it onto the original dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing pickup and dropoff locations.\n",
    "    - n_clusters (int): The number of clusters for KMeans.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The modified DataFrame with cluster labels and average cluster duration.\n",
    "    \"\"\"\n",
    "    # fit the kmeans models and label each pickup and dropoff location by its cluster\n",
    "    kmeans_pickup = (KMeans(n_clusters=n_clusters)\n",
    "        .fit(df.loc[:, ['pickup_longitude', 'pickup_latitude']].values))\n",
    "    kmeans_dropoff = (KMeans(n_clusters=n_clusters)\n",
    "        .fit(df.loc[:, ['dropoff_longitude', 'dropoff_latitude']].values))\n",
    "    df['pickup_cluster'] = kmeans_pickup.predict(df[['pickup_longitude', 'pickup_latitude']].values)\n",
    "    df['dropoff_cluster'] = kmeans_dropoff.predict(df[['dropoff_longitude', 'dropoff_latitude']].values)\n",
    "\n",
    "    # compute the average duration between each cluster and merge this onto the original dataframe\n",
    "    group_durations = (df\n",
    "        .groupby(['pickup_cluster', 'dropoff_cluster'])['trip_duration']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'trip_duration': 'avg_cluster_duration'}))\n",
    "    df = pd.merge(\n",
    "        left=df, right=group_durations, how='left',\n",
    "        left_on=['pickup_cluster', 'dropoff_cluster'], right_on=['pickup_cluster', 'dropoff_cluster'])\n",
    "\n",
    "    # fill the missing values with the mean of the average duration from cluster to cluster\n",
    "    df['avg_cluster_duration'] = df['avg_cluster_duration'].fillna(df['avg_cluster_duration'].mean())\n",
    "    df.drop(columns=['pickup_200_cluster', 'dropoff_200_cluster', 'trip_duration'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Clustering Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants/parameters for this code cell\n",
    "SHOW_PLOTS = True\n",
    "LOAD_SAVED_KMEANS_MODELS = True\n",
    "\n",
    "# load in the cleaned training data and the NYC geopandas dataframe\n",
    "# with all of the NYC streets\n",
    "X, y = get_X_y(force_clean=True)\n",
    "nyc_gdf = get_nyc_gdf()\n",
    "\n",
    "\n",
    "#########################\n",
    "# PLOT PICKUP LOCATIONS #\n",
    "#########################\n",
    "def plot_pickup_locations(X):\n",
    "    \"\"\"\n",
    "    Plots the NYC streets and pickup locations as a scatter plot on top of the streets.\n",
    "\n",
    "    Parameters:\n",
    "    - X (DataFrame): The DataFrame containing pickup locations.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # plot the nyc streets\n",
    "    plt.gcf()\n",
    "    nyc_gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "    # plot the pickup locations as a scatter plot on top of the nyc streets\n",
    "    plt.scatter(X['pickup_longitude'], X['pickup_latitude'], c='red', alpha=0.75, s=0.1, label=\"Pickup Locations\")\n",
    "    leg = plt.legend(loc='upper left')\n",
    "    for lh in leg.legend_handles: \n",
    "        lh.set_alpha(1)\n",
    "    plt.title(\"Pickup Locations\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    # save the plot\n",
    "    plt.savefig(\"images/pickup_locations_save.png\")\n",
    "    plt.show() if SHOW_PLOTS else plt.clf()\n",
    "\n",
    "\n",
    "##########################\n",
    "# PLOT DROPOFF LOCATIONS #\n",
    "##########################\n",
    "def plot_dropoff_locations(X):\n",
    "    \"\"\"\n",
    "    Plots the NYC streets and dropoff locations as a scatter plot on top of the streets.\n",
    "\n",
    "    Parameters:\n",
    "    - X (DataFrame): The DataFrame containing dropoff locations.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # plot the nyc streets\n",
    "    plt.gcf()\n",
    "    nyc_gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "    # plot the dropoff locations as a scatter plot on top of the nyc streets\n",
    "    plt.scatter(X['dropoff_longitude'], X['dropoff_latitude'], c='green', alpha=0.75, s=0.1, label=\"Dropoff Locations\")\n",
    "    leg = plt.legend(loc='upper left')\n",
    "    for lh in leg.legend_handles: \n",
    "        lh.set_alpha(1)\n",
    "    plt.title(\"Dropoff Locations\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    # save the plot\n",
    "    plt.savefig(\"images/dropoff_locations_save.png\")\n",
    "    plt.show() if SHOW_PLOTS else plt.clf()\n",
    "\n",
    "\n",
    "#####################\n",
    "# KMEANS CLUSTERING #\n",
    "#####################\n",
    "def kmeans_pickup_dropoff_predict(df, n_clusters=200):\n",
    "    \"\"\"\n",
    "    Applies KMeans clustering to predict pickup and dropoff locations, or loads pre-trained models if available.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing pickup and dropoff locations.\n",
    "    - n_clusters (int): The number of clusters for KMeans. Default is 200.\n",
    "\n",
    "    Returns:\n",
    "    - df (DataFrame): The DataFrame with predicted clusters for pickup and dropoff locations.\n",
    "    - pickup_200_centers (array): The cluster centers for pickup locations.\n",
    "    - dropoff_200_centers (array): The cluster centers for dropoff locations.\n",
    "    \"\"\"\n",
    "    df = deepcopy(X)\n",
    "\n",
    "    # load kmeans_pickup and kmeans_dropoff from the models folder using pickle\n",
    "    if LOAD_SAVED_KMEANS_MODELS:\n",
    "        with open(\"models/kmeans_200_pickup.pkl\", \"rb\") as file:\n",
    "            kmeans_200_pickup = pickle.load(file)\n",
    "        with open(\"models/kmeans_200_dropoff.pkl\", \"rb\") as file:\n",
    "            kmeans_200_dropoff = pickle.load(file)\n",
    "        \n",
    "            \n",
    "    # fit kmeans_pickup and kmeans_dropoff with 200 clusters\n",
    "    else:\n",
    "        n_clusters = 200\n",
    "        kmeans_pickup = (KMeans(n_clusters=n_clusters)\n",
    "            .fit(df.loc[:, ['pickup_longitude', 'pickup_latitude']].values))\n",
    "        kmeans_dropoff = (KMeans(n_clusters=n_clusters)\n",
    "            .fit(df.loc[:, ['dropoff_longitude', 'dropoff_latitude']].values))\n",
    "        \n",
    "        # save the models to pickle files for loading later\n",
    "        with open(\"models/kmeans_200_pickup.pkl\", \"wb\") as file:\n",
    "            pickle.dump(kmeans_pickup, file)\n",
    "        with open(\"models/kmeans_200_dropoff.pkl\", \"wb\") as file:\n",
    "            pickle.dump(kmeans_dropoff, file)\n",
    "\n",
    "    # predict the clusters for each pickup and dropoff location\n",
    "    df['pickup_200_cluster'] = kmeans_200_pickup.predict(df[['pickup_longitude', 'pickup_latitude']].values)\n",
    "    df['dropoff_200_cluster'] = kmeans_200_dropoff.predict(df[['dropoff_longitude', 'dropoff_latitude']].values)\n",
    "\n",
    "    # get the centers\n",
    "    pickup_200_centers = kmeans_200_pickup.cluster_centers_\n",
    "    dropoff_200_centers = kmeans_200_dropoff.cluster_centers_\n",
    "    return df, pickup_200_centers, dropoff_200_centers\n",
    "\n",
    "\n",
    "#######################################\n",
    "# PLOT PICKUP LOCATIONS WITH CLUSTERS #\n",
    "#######################################\n",
    "def plot_cluster_pickup(df, pickup_200_centers):\n",
    "    \"\"\"\n",
    "    Plots KMeans clustering for pickup locations.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing pickup locations and their associated clusters.\n",
    "    - pickup_200_centers (array): The cluster centers for pickup locations.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # plot the nyc streets\n",
    "    plt.gcf()\n",
    "    nyc_gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "    # plot the cluster locations and the pickup locations color-coded\n",
    "    # to their associated cluster\n",
    "    plt.scatter(df['pickup_longitude'], df['pickup_latitude'], c=df['pickup_200_cluster'], cmap='magma', alpha=1.0, s=0.1, label=\"Pickup Locations\")\n",
    "    plt.scatter(pickup_200_centers[:, 0], pickup_200_centers[:, 1], c='red', alpha=1, s=10, label=\"Cluster Centers\")\n",
    "    leg = plt.legend(loc='upper left')\n",
    "    for lh in leg.legend_handles: \n",
    "        lh.set_alpha(1)\n",
    "    plt.title(\"200-KMeans Clustering for Pickup Locations\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    # save the plot\n",
    "    plt.savefig(\"images/kmeans_200_pickup_save.png\")\n",
    "    plt.show() if SHOW_PLOTS else plt.clf()\n",
    "\n",
    "\n",
    "########################################\n",
    "# PLOT DROPOFF LOCATIONS WITH CLUSTERS #\n",
    "########################################\n",
    "def plot_cluster_dropoff(df, dropoff_200_centers):\n",
    "    \"\"\"\n",
    "    Plots KMeans clustering for dropoff locations.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing dropoff locations and their associated clusters.\n",
    "    - dropoff_200_centers (array): The cluster centers for dropoff locations.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # plot the nyc streets\n",
    "    plt.gcf()\n",
    "    nyc_gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "    # plot the cluster locations and the pickup locations color-coded\n",
    "    # to their associated cluster\n",
    "    plt.scatter(df['dropoff_longitude'], df['dropoff_latitude'], c=df['dropoff_200_cluster'], cmap='viridis', alpha=1.0, s=0.1, label=\"Dropoff Locations\")\n",
    "    plt.scatter(dropoff_200_centers[:, 0], dropoff_200_centers[:, 1], c='blue', alpha=1, s=10, label=\"Cluster Centers\")\n",
    "    leg = plt.legend(loc='upper left')\n",
    "    for lh in leg.legend_handles: \n",
    "        lh.set_alpha(1)\n",
    "    plt.title(\"200-KMeans Clustering for Dropoff Locations\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    # save the plot\n",
    "    plt.savefig(\"images/kmeans_200_dropoff_save.png\")\n",
    "    plt.show() if SHOW_PLOTS else plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted_distributions(show=False):\n",
    "    \"\"\"\n",
    "    Plots histograms of actual and predicted trip durations on the test set and compares their distributions.\n",
    "    \n",
    "    Parameters:\n",
    "    - show (bool): If True, displays the plot; if False, saves the plot as 'images/actual_vs_predicted.png'.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    # load in the actual and predicted trip durations\n",
    "    X, y = get_X_y()\n",
    "    feature_X = generate_features(X, y)\n",
    "    feature_X = feature_X.drop(columns=['pickup_datetime'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # plot a histogram of the actual trip duration distribution in the test set\n",
    "    # and compute the mean\n",
    "    counts, bins, patches = plt.hist(y_test, label='Actual Trip Durations')\n",
    "    actual_mean = np.mean(y_test)\n",
    "\n",
    "    # plot a histogram of the predicted trip duration distribution on the test set\n",
    "    counts_pred, bins_pred, pathces_pred = plt.hist(y_pred, label='Predicted Trip Durations', color='orange')\n",
    "\n",
    "    # plot a verticle line at the actual mean of the distribution\n",
    "    top = max(np.max(counts), np.max(counts_pred))\n",
    "    plt.vlines([actual_mean], 0, top, color='black', linestyles='dashed', label='Actual Mean')\n",
    "\n",
    "    # set other plot parameters and show the plot\n",
    "    plt.legend()\n",
    "    plt.title(\"Actual and Estimated Distribution of Trip Durations\")\n",
    "    plt.xlabel(\"Trip Duration (seconds)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig('images/actual_vs_predicted.png')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "def plot_heat_map(show=False):\n",
    "    \"\"\"\n",
    "    Generate and save a heatmap of pickup frequency by day and hour.\n",
    "\n",
    "    Parameters:\n",
    "    - show (bool): If True, display the plot; if False (default), save the plot without displaying.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Load in the actual and predicted trip durations\n",
    "    X, y = get_X_y()\n",
    "    feature_X = generate_features(X, y)\n",
    "    \n",
    "    # Create new column for day of the week\n",
    "    feature_X['day_of_week'] = feature_X['pickup_datetime'].dt.day_name()\n",
    "\n",
    "    # Define the order of days\n",
    "    day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "\n",
    "    # Create a pivot table for day_of_week and pickup_hour\n",
    "    pivot_table = feature_X.pivot_table(index='day_of_week', columns='pickup_hour', aggfunc='size', fill_value=0)\n",
    "    \n",
    "    # Sort index and columns for better visualization\n",
    "    pivot_table.sort_index(ascending=False, inplace=True)\n",
    "    pivot_table = pivot_table.reindex(day_order)\n",
    "\n",
    "    # Create the heatmap using seaborn\n",
    "    plt.figure(figsize=(10, 4), dpi=200)\n",
    "    sns.heatmap(pivot_table, cmap='magma', fmt='d', cbar_kws={'label': 'Frequency'})\n",
    "    plt.title('Pickup Frequency by Day and Hour')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Day of Week')\n",
    "    plt.savefig('images/day_hour.png')\n",
    "    \n",
    "    # Show or save the plot based on the 'show' parameter\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "def plot_residual(y_actual, y_pred, title=\"Residuals for [Model Name]\", save_postfix='model', show=False):\n",
    "    \"\"\"\n",
    "    Plot and optionally save the residuals of a regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - y_actual (array-like): The actual values.\n",
    "    - y_pred (array-like): The predicted values.\n",
    "    - title (str, optional): Title for the plot. Default is \"Residuals for [Model Name]\".\n",
    "    - save_postfix (str, optional): A postfix to add to the saved image filename. Default is 'model'.\n",
    "    - show (bool, optional): If True, display the plot; if False (default), save the plot without displaying.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Compute the residuals\n",
    "    residual = y_actual - y_pred\n",
    "    \n",
    "    # Plot the residuals with the x-axis as the predictions and the y-axis\n",
    "    # as the residuals (y_actual - y_pred)\n",
    "    plt.scatter(y_pred, residual, alpha=0.1, label=\"Residuals\")\n",
    "    \n",
    "    # Plot the residual=0 line\n",
    "    lower = np.min(y_pred)\n",
    "    upper = np.max(y_pred)\n",
    "    ls = np.linspace(lower, upper, 100)\n",
    "    plt.plot(ls, np.zeros(len(ls)), color='black', label='Residual=0 line')\n",
    "    \n",
    "    # Plot the mean of the actual data\n",
    "    plt.vlines([np.mean(y_actual)], ymin=np.min(residual), ymax=np.max(residual), linestyles='dashed', color='red', label='Mean of y_actual')\n",
    "    \n",
    "    # Setup other plot parameters and show or save the plot\n",
    "    plt.xlabel(\"Prediction $\\widehat{y}$\")\n",
    "    plt.ylabel(\"Residual ( $y$ - $\\widehat{y}$ )\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'images/residual_plot_{save_postfix}.png')\n",
    "    \n",
    "    # Show or save the plot based on the 'show' parameter\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_per_distance():\n",
    "    \"\"\"\n",
    "    Analyzes the relationship between trip duration and distance between pickup and dropoff locations\n",
    "    for NYC taxi data using clustering and visualization.\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Retrieve data and preprocess\n",
    "    X, y = get_X_y()\n",
    "    X = generate_features(X, y)\n",
    "    full_df = pd.concat([X, y], axis=1)\n",
    "    nyc_gdf = get_nyc_gdf()\n",
    "    \n",
    "    # create a copy of the dataframe\n",
    "    df = deepcopy(full_df)\n",
    "    \n",
    "    # Load KMeans models for pickup and dropoff locations\n",
    "    with open(\"models/kmeans_200_pickup.pkl\", \"rb\") as file:\n",
    "        kmeans_200_pickup = pickle.load(file)\n",
    "    with open(\"models/kmeans_200_dropoff.pkl\", \"rb\") as file:\n",
    "        kmeans_200_dropoff = pickle.load(file)\n",
    "        \n",
    "    # Predict the clusters for pickup and dropoff locations\n",
    "    df['pickup_200_cluster'] = kmeans_200_pickup.predict(df[['pickup_longitude', 'pickup_latitude']].values)\n",
    "    df['dropoff_200_cluster'] = kmeans_200_dropoff.predict(df[['dropoff_longitude', 'dropoff_latitude']].values)\n",
    "\n",
    "    # Retrieve cluster centers\n",
    "    pickup_200_centers = kmeans_200_pickup.cluster_centers_\n",
    "    dropoff_200_centers = kmeans_200_dropoff.cluster_centers_\n",
    "    \n",
    "    # Create DataFrames for cluster coordinates\n",
    "    pickup_cluster_df = pd.DataFrame({\n",
    "        'pickup_200_cluster': np.arange(0, 200),\n",
    "        'pickup_longitude': pickup_200_centers[:, 0],\n",
    "        'pickup_latitude': pickup_200_centers[:, 1]\n",
    "    })\n",
    "    dropoff_cluster_df = pd.DataFrame({\n",
    "        'dropoff_200_cluster': np.arange(0, 200),\n",
    "        'dropoff_longitude': dropoff_200_centers[:, 0],\n",
    "        'dropoff_latitude': dropoff_200_centers[:, 1]\n",
    "    })\n",
    "    \n",
    "    # compute the average duration from cluster to cluster\n",
    "    group_durations = (df\n",
    "        .groupby(['pickup_200_cluster', 'dropoff_200_cluster'])['trip_duration']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'trip_duration': 'avg_cluster_duration'}))\n",
    "    \n",
    "    # Sort and merge cluster durations and coordinates\n",
    "    group_durations_sorted = group_durations.sort_values(by='avg_cluster_duration', ascending=True)\n",
    "    cluster_dist = group_durations_sorted.copy()\n",
    "\n",
    "    cluster_dist = pd.merge(\n",
    "        left=cluster_dist, right=pickup_cluster_df, how='left',\n",
    "        left_on=['pickup_200_cluster'], right_on=['pickup_200_cluster'])\n",
    "    cluster_dist = pd.merge(\n",
    "        left=cluster_dist, right=dropoff_cluster_df, how='left',\n",
    "        left_on=['dropoff_200_cluster'], right_on=['dropoff_200_cluster'])\n",
    "\n",
    "    # Calculate distances between pickup and dropoff locations\n",
    "    pickup_longitudes = cluster_dist['pickup_longitude'].values\n",
    "    pickup_latitudes = cluster_dist['pickup_latitude'].values\n",
    "    dropoff_longitudes = cluster_dist['dropoff_longitude'].values\n",
    "    dropoff_latitudes = cluster_dist['dropoff_latitude'].values\n",
    "\n",
    "    cluster_dist['distance'] = np.sqrt((pickup_longitudes - dropoff_longitudes)**2 + (pickup_latitudes - dropoff_latitudes)**2)\n",
    "    cluster_dist['duration_dist_ratio'] = cluster_dist['avg_cluster_duration'] / cluster_dist['distance']\n",
    "    cluster_dist = cluster_dist.sort_values(by='duration_dist_ratio', ascending=False)\n",
    "\n",
    "    # Retrieve and plot clusters with the worst traffic\n",
    "    n = 1000\n",
    "    worst_traffic = cluster_dist.iloc[:n, :]\n",
    "\n",
    "    pickup_longitudes = worst_traffic['pickup_longitude'].values\n",
    "    pickup_latitudes = worst_traffic['pickup_latitude'].values\n",
    "    dropoff_longitudes = worst_traffic['dropoff_longitude'].values\n",
    "    dropoff_latitudes = worst_traffic['dropoff_latitude'].values\n",
    "\n",
    "    # plot the nyc streets\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(12, 12)\n",
    "\n",
    "    nyc_gdf.plot(ax=ax, linewidth=0.1, edgecolor='black', alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "    # plot the cluster locations and the pickup locations color-coded\n",
    "    plt.scatter(pickup_longitudes, pickup_latitudes, c='blue', alpha=1.0, s=10, label=\"Pickup Locations\")\n",
    "    plt.scatter(dropoff_longitudes, dropoff_latitudes, c='red', alpha=1.0, s=10, label=\"Dropoff Locations\")\n",
    "        \n",
    "    # plot the lines between pickup and dropoff locations\n",
    "    for plong, plat, dlong, dlat in zip(pickup_longitudes, pickup_latitudes, dropoff_longitudes, dropoff_latitudes):\n",
    "        x = np.linspace(plong, dlong, 100)\n",
    "        y = np.linspace(plat, dlat, 100)\n",
    "        cols = np.linspace(0, 1, 100)\n",
    "        points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "                                \n",
    "        lc = LineCollection(segments, cmap='coolwarm')\n",
    "        lc.set_array(cols)\n",
    "        lc.set_linewidth(2)\n",
    "        ax.add_collection(lc)\n",
    "    \n",
    "    # set other plot parameters and show the plot\n",
    "    plt.title(f\"{n} Clusters with longest average trip duration per distance\", fontsize=20)\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.xlabel(\"Longitude\", fontsize=20)\n",
    "    plt.ylabel(\"Latitude\", fontsize=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDE for Hour and Weather Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kde(X, show=False):\n",
    "    \"\"\"\n",
    "    Plot Kernel Density Estimate (KDE) plots for pickup frequency by hour based on weather conditions.\n",
    "\n",
    "    Parameters:\n",
    "    - X (DataFrame): Input DataFrame containing features, including 'precipitation (mm)' and 'pickup_hour'.\n",
    "    - show (bool, optional): If True, the plot is displayed interactively. If False, the plot is saved to an image file.\n",
    "                             Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # get all rows where precipitation (mm) = 0\n",
    "    sunny = X[X['precipitation (mm)'] == 0]['pickup_hour']\n",
    "    precip = X[X['precipitation (mm)'] > 0]['pickup_hour']\n",
    "\n",
    "    # plot the kde plots\n",
    "    plt.figure(figsize=(8, 5), dpi=200)\n",
    "    sbn.kdeplot(precip, shade=True, label='Rainy', color='blue', bw_adjust=2)\n",
    "    sbn.kdeplot(sunny, shade=True, label='Sunny', color='orange', bw_adjust=2)\n",
    "\n",
    "    # set other plot parameters\n",
    "    plt.title('Pickup Frequency by Hour and Weather')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig('images/kde_plot.png', dpi=200)\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this py file contains all of the data loading, cleaning, and saving logic.\n",
    "The methods will automatically pull in a cached version of the dataframe\n",
    "unless force_clean=True. If force_clean=True, then the dataframe will be\n",
    "cleaned and saved to the data folder.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import geopandas as gpd\n",
    "from shapely.wkt import loads\n",
    "\n",
    "from config import (\n",
    "    data_path, cols_to_drop, SET_VENDOR_ID_TO_01,\n",
    "    PICKUP_TIME_TO_NORMALIZED_FLOAT\n",
    ")\n",
    "from py_files.helper_funcs import p\n",
    "\n",
    "\n",
    "def clean_data(df, df_name, verbose=False):\n",
    "    \"\"\"\n",
    "    Loads in the train.csv and test.csv and cleans them according\n",
    "    to the constants in config.py. Saves the cleaned dataframes as\n",
    "    train_clean.csv and test_clean.csv\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas dataframe): The dataframe to be cleaned\n",
    "    - df_name (str): The name of the dataframe, either 'train' or 'test'\n",
    "    - verbose (bool): If True, prints out the progress of the cleaning\n",
    "\n",
    "    Returns:\n",
    "    - df_clean (pandas dataframe): The cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # only keep the relevant columns based on the config\n",
    "    p(\"dropping columns\") if verbose else None\n",
    "    curr_cols_to_drop = [c for c in df.columns if c in cols_to_drop]\n",
    "    df_clean = df.drop(columns=curr_cols_to_drop)\n",
    "    p() if verbose else None\n",
    "\n",
    "    # setting vendor_id to a 0 or 1 instead of 1 and 2\n",
    "    if SET_VENDOR_ID_TO_01:\n",
    "        p(\"setting vendor_id to 0 or 1\") if verbose else None\n",
    "        df_clean['vendor_id'] = df_clean['vendor_id'] - 1\n",
    "        p() if verbose else None\n",
    "\n",
    "    # Drop rows with trip duration < 60 seconds\n",
    "    p(\"dropping rows with trip duration < 60 seconds\") if verbose else None\n",
    "    df_clean = df_clean[df_clean['trip_duration'] >= 60]\n",
    "\n",
    "    # Drop rows with outlier locations\n",
    "    p(\"dropping rows with outlier locations\") if verbose else None\n",
    "    json_file_path = './misc/lat_long_bounds.json'\n",
    "    # Read in coordinates\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        # Load the JSON data from the file\n",
    "        coords = json.load(json_file)\n",
    "    df_clean = df_clean[(df_clean['pickup_latitude'] >= coords['lat']['min']) & (\n",
    "        df_clean['pickup_latitude'] <= coords['lat']['max'])]\n",
    "    df_clean = df_clean[(df_clean['pickup_longitude'] >= coords['lon']['min']) & (\n",
    "        df_clean['pickup_longitude'] <= coords['lon']['max'])]\n",
    "    df_clean = df_clean[(df_clean['dropoff_latitude'] >= coords['lat']['min']) & (\n",
    "        df_clean['dropoff_latitude'] <= coords['lat']['max'])]\n",
    "    df_clean = df_clean[(df_clean['dropoff_longitude'] >= coords['lon']['min']) & (\n",
    "        df_clean['dropoff_longitude'] <= coords['lon']['max'])]\n",
    "\n",
    "    # Keep only <99.5% of trip duration\n",
    "    p(\"dropping rows with trip duration > 99.5%\") if verbose else None\n",
    "    df_clean = df_clean[df_clean['trip_duration'] <=\n",
    "                        df_clean['trip_duration'].quantile(0.995)]\n",
    "\n",
    "    # Split apart pickup_datetime\n",
    "    df_clean['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df_clean['pickup_month'] = df_clean['pickup_datetime'].dt.month\n",
    "    df_clean['pickup_day'] = df_clean['pickup_datetime'].dt.day_name()\n",
    "    df_clean = pd.get_dummies(\n",
    "        df_clean, columns=['pickup_day'], drop_first=True)\n",
    "    df_clean['pickup_hour'] = df_clean['pickup_datetime'].dt.hour\n",
    "    df_clean['pickup_minute'] = df_clean['pickup_datetime'].dt.minute\n",
    "\n",
    "    # Create a pickup period.\n",
    "    df_clean['pickup_period'] = pd.cut(df_clean['pickup_hour'], bins=[\n",
    "                                       -1, 6, 12, 18, 24], labels=['night', 'morning', 'afternoon', 'evening'])\n",
    "\n",
    "    # Get dummies for the pickup period.\n",
    "    df_clean = pd.get_dummies(\n",
    "        df_clean, columns=['pickup_period'], drop_first=True)\n",
    "\n",
    "    # Add cyclic data.\n",
    "    df_clean['pickup_hour_sin'] = np.sin(\n",
    "        2 * np.pi * df_clean['pickup_hour'] / 24)\n",
    "    df_clean['pickup_hour_cos'] = np.cos(\n",
    "        2 * np.pi * df_clean['pickup_hour'] / 24)\n",
    "\n",
    "    # convert pickup and dropoff times to floats from 0 to 1\n",
    "    if PICKUP_TIME_TO_NORMALIZED_FLOAT:\n",
    "        df_clean['pickup_datetime_norm'] = pd.to_datetime(\n",
    "            df_clean['pickup_datetime']).view('int64') // 10**9\n",
    "        df_clean['pickup_datetime_norm'] = (df_clean['pickup_datetime_norm'] - df_clean['pickup_datetime_norm'].min()) / (\n",
    "            df_clean['pickup_datetime_norm'].max() - df_clean['pickup_datetime_norm'].min())\n",
    "    \n",
    "    # Drop the id column\n",
    "    df_clean = df_clean.drop(columns=['id'])\n",
    "\n",
    "    # save the cleaned dataframe\n",
    "    p(\"saving cleaned dataframe\") if verbose else None\n",
    "    df_clean.to_csv(f\"{data_path}/{df_name}_clean.csv\", index=False)\n",
    "    p() if verbose else None\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def get_train_data(force_clean=False):\n",
    "    \"\"\"\n",
    "    Either creates the cleaned train dataframe from the train.csv\n",
    "    or it loads it from the data folder\n",
    "\n",
    "    Parameters:\n",
    "    - force_clean (bool): If True, forces the data to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    - train (pandas dataframe): A dataframe of the train data\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f\"{data_path}/train_clean.csv\") or force_clean:\n",
    "        train = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "        return clean_data(train, 'train')\n",
    "    else:\n",
    "        return pd.read_csv(f\"{data_path}/train_clean.csv\")\n",
    "\n",
    "\n",
    "def get_X_y(return_np=False, force_clean=False):\n",
    "    \"\"\"\n",
    "    Returns the X and y dataframes from a dataframe\n",
    "\n",
    "    Parameters:\n",
    "    - return_np (bool): If True, returns numpy arrays instead of\n",
    "    - force_clean (bool): If True, forces the data to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    - X (pandas dataframe): A dataframe of the input data\n",
    "    - y (pandas dataframe): A dataframe of the label data\n",
    "    \"\"\"\n",
    "    df = get_train_data(force_clean=force_clean)\n",
    "    X = df.drop(columns=['trip_duration'])\n",
    "    y = df['trip_duration']\n",
    "\n",
    "    if return_np:\n",
    "        X, y = X.values, y.values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_test_data():\n",
    "    \"\"\"\n",
    "    Either creates the cleaned test dataframe from the test.csv\n",
    "    or it loads it from the data folder\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - test (pandas dataframe): A dataframe of the test data\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f\"{data_path}/test_clean.csv\"):\n",
    "        test = pd.read_csv(f\"{data_path}/test.csv\")\n",
    "        return clean_data(test, 'test')\n",
    "    else:\n",
    "        return pd.read_csv(f\"{data_path}/test_clean.csv\")\n",
    "\n",
    "\n",
    "def get_clean_weather():\n",
    "    \"\"\"\n",
    "    Loads in the NYC_Weather_2016_2022.csv and cleans it according\n",
    "    to the constants in config.py. Saves the cleaned dataframe as\n",
    "    weather_clean.csv\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - weather (pandas dataframe): A dataframe of the weather\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f\"{data_path}/weather_clean1.csv\"):\n",
    "        weather = pd.read_csv(f\"{data_path}/NYC_Weather_2016_2022.csv\")\n",
    "        weather = weather.dropna()\n",
    "        weather['time'] = pd.to_datetime(weather['time'])\n",
    "        weather = weather[weather['time'] <= '2016-07-01']\n",
    "        weather = weather.drop(columns=['rain (mm)',\n",
    "                                        'cloudcover_low (%)',\n",
    "                                        'cloudcover_mid (%)',\n",
    "                                        'cloudcover_high (%)',\n",
    "                                        'windspeed_10m (km/h)',\n",
    "                                        'winddirection_10m (Â°)'])\n",
    "        weather.to_csv(f\"{data_path}/weather_clean1.csv\", index=False)\n",
    "        return weather\n",
    "    else:\n",
    "        return pd.read_csv(f\"{data_path}/weather_clean1.csv\")\n",
    "\n",
    "\n",
    "def get_google_distance():\n",
    "    \"\"\"\n",
    "    Loads in the train_distance_matrix.csv and cleans it according\n",
    "    to the constants in config.py. Saves the cleaned dataframe as\n",
    "    google_distance_clean.csv\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - google_distance (pandas dataframe): A dataframe of the google\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f\"{data_path}/google_distance_clean.csv\"):\n",
    "        google_distance = pd.read_csv(f\"{data_path}/train_distance_matrix.csv\")\n",
    "\n",
    "        columns_to_keep = ['id', 'gc_distance', 'google_distance']\n",
    "        google_distance = google_distance[columns_to_keep]\n",
    "\n",
    "        google_distance.to_csv(\n",
    "            f\"{data_path}/google_distance_clean.csv\", index=False)\n",
    "        return google_distance\n",
    "    else:\n",
    "        return pd.read_csv(f\"{data_path}/google_distance_clean.csv\")\n",
    "    \n",
    "    \n",
    "def get_nyc_gdf():\n",
    "    \"\"\"\n",
    "    Loads in the NYC street centerline data and returns it as a\n",
    "    geopandas dataframe\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - gdf (geopandas dataframe): A geopandas dataframe of the NYC\n",
    "    \"\"\"\n",
    "    nyc_df = pd.read_csv(f\"{data_path}/Centerline.csv\")\n",
    "    nyc_df = nyc_df.loc[:, ['the_geom']]\n",
    "\n",
    "    # Convert the \"the_geom\" column to Shapely geometries\n",
    "    nyc_df['the_geom_geopandas'] = nyc_df['the_geom'].apply(loads)\n",
    "\n",
    "    # Create a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(nyc_df, geometry='the_geom_geopandas')\n",
    "    \n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this py file holds all of the logic behind the feature engineering.\n",
    "The generate_features function takes in an X and a y, and it adds\n",
    "feature columns based on the config.features_toggle\n",
    "\"\"\"\n",
    "\n",
    "from config import features_toggle\n",
    "from py_files.data_manager import get_clean_weather, get_google_distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "def distance(df):\n",
    "    \"\"\"\n",
    "    Calculate the Manhattan distance in kilometers between pickup and dropoff locations\n",
    "    and add it as a new column 'distance_km' to the DataFrame.\n",
    "\n",
    "    The Manhattan distance, also known as the L1 distance or taxicab distance, between two points\n",
    "    on the Earth's surface is calculated by finding the absolute differences between their respective\n",
    "    longitudes and latitudes and summing them up. This function computes the Manhattan distance\n",
    "    in kilometers between the pickup and dropoff locations in a DataFrame, assuming a constant\n",
    "    Earth radius of 6371 kilometers.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): A DataFrame containing pickup and dropoff coordinates with columns\n",
    "                           'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', and 'dropoff_latitude'.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame with an additional 'distance_km' column representing the Manhattan\n",
    "                     distance in kilometers between pickup and dropoff locations.\n",
    "    \"\"\"\n",
    "    # Radius of the Earth in kilometers\n",
    "    earth_radius_km = 6371.0\n",
    "\n",
    "    # Get the pickup and dropoff coordinates\n",
    "    lon1 = df['pickup_longitude']\n",
    "    lat1 = df['pickup_latitude']\n",
    "    lon2 = df['dropoff_longitude']\n",
    "    lat2 = df['dropoff_latitude']\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "\n",
    "    # Calculate the differences in latitude and longitude\n",
    "    delta_lat = abs(lat1 - lat2)\n",
    "    delta_lon = abs(lon1 - lon2)\n",
    "\n",
    "    # Calculate the Manhattan distance in kilometers\n",
    "    df['distance_km'] = earth_radius_km * (delta_lat + delta_lon)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_weather_feature(df):\n",
    "    \"\"\"\n",
    "    Add weather-related features to a DataFrame by merging it with a weather dataset.\n",
    "\n",
    "    This function merges the input DataFrame with a weather dataset based on the rounded pickup time\n",
    "    and adds weather-related features to the DataFrame. It rounds the 'pickup_datetime' column to the\n",
    "    nearest hour to match the weather data's time resolution.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The input DataFrame containing pickup-related data.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame with added weather-related features merged from the weather dataset.\n",
    "    \"\"\"\n",
    "    # Get weather data\n",
    "    weather = get_clean_weather()\n",
    "    \n",
    "    # Round the pickup time to the nearest hour (to merge with weather)\n",
    "    df['rounded_date'] = pd.to_datetime(df['pickup_datetime']).dt.round('H')\n",
    "    weather['time'] = pd.to_datetime(weather['time'])\n",
    "\n",
    "    # Merge with weather\n",
    "    df = df.merge(weather, left_on='rounded_date', right_on='time')\n",
    "\n",
    "    # Drop unnecessary columns and return the dataframe\n",
    "    df = df.drop(columns=['rounded_date', 'time'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_google_distance(df):\n",
    "    \"\"\"\n",
    "    Add Google Maps distance and duration features to a DataFrame by merging it with a Google Maps dataset.\n",
    "    \n",
    "    Parameters\n",
    "    - df (DataFrame): The input DataFrame containing pickup and dropoff coordinates.\n",
    "    \n",
    "    Returns\n",
    "    - df (DataFrame): The DataFrame with added Google Maps distance and duration features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the google distance\n",
    "    google_distance = get_google_distance()\n",
    "\n",
    "    # Merge with the dataframe (verify 1:1)\n",
    "    df = df.merge(google_distance, on='id', validate='1:1')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_avg_cluster_duration(df, y):\n",
    "    \"\"\"\n",
    "    Adds a column 'avg_cluster_duration' to the DataFrame representing the average duration \n",
    "    from cluster to cluster based on pickup and dropoff locations.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing features.\n",
    "    - y (array-like): The array of target values (trip durations).\n",
    "\n",
    "    Returns:\n",
    "    - df (DataFrame): The DataFrame with added 'avg_cluster_duration' column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['trip_duration'] = y\n",
    "    \n",
    "    # load kmeans_pickup and kmeans_dropoff from the models folder using pickle\n",
    "    with open(\"models/kmeans_200_pickup.pkl\", \"rb\") as file:\n",
    "        kmeans_200_pickup = pickle.load(file)\n",
    "    with open(\"models/kmeans_200_dropoff.pkl\", \"rb\") as file:\n",
    "        kmeans_200_dropoff = pickle.load(file)\n",
    "        \n",
    "    # predict the clusters for the pickup and dropoff locations using the kmeans_pickup and kmeans_dropoff\n",
    "    df['pickup_200_cluster'] = kmeans_200_pickup.predict(df[['pickup_longitude', 'pickup_latitude']].values)\n",
    "    df['dropoff_200_cluster'] = kmeans_200_dropoff.predict(df[['dropoff_longitude', 'dropoff_latitude']].values)\n",
    "\n",
    "    # get the centers\n",
    "    pickup_200_centers = kmeans_200_pickup.cluster_centers_\n",
    "    dropoff_200_centers = kmeans_200_dropoff.cluster_centers_\n",
    "\n",
    "    # compute the average duration from cluster to cluster\n",
    "    group_durations = (df\n",
    "        .groupby(['pickup_200_cluster', 'dropoff_200_cluster'])['trip_duration']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'trip_duration': 'avg_cluster_duration'}))\n",
    "\n",
    "    # merge the average duration from cluster to cluster with the main dataframe\n",
    "    df = pd.merge(\n",
    "        left=df, right=group_durations, how='left',\n",
    "        left_on=['pickup_200_cluster', 'dropoff_200_cluster'], right_on=['pickup_200_cluster', 'dropoff_200_cluster'])\n",
    "\n",
    "    # fill the missing values with the mean of the average duration from cluster to cluster\n",
    "    df['avg_cluster_duration'] = df['avg_cluster_duration'].fillna(df['avg_cluster_duration'].mean())\n",
    "    df.drop(columns=['pickup_200_cluster', 'dropoff_200_cluster', 'trip_duration'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_features(df, y=None):\n",
    "    \"\"\"\n",
    "    Generates additional features based on the specified feature toggles and appends them to the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing base features.\n",
    "    - y (array-like, optional): The array of target values. Required if 'avg_cluster_duration' feature is enabled.\n",
    "\n",
    "    Returns:\n",
    "    - feature_df (DataFrame): The DataFrame with added features.\n",
    "    \"\"\"\n",
    "    # append the features to the dataframe\n",
    "    feature_df = df\n",
    "\n",
    "    # add the distance feature\n",
    "    if features_toggle['distance']:\n",
    "        feature_df = distance(feature_df)\n",
    "    \n",
    "    # add the weather feature\n",
    "    if features_toggle['weather']:\n",
    "        feature_df = add_weather_feature(feature_df)\n",
    "\n",
    "    # add the google distance feature\n",
    "    if features_toggle['google_distance']:\n",
    "        feature_df = add_google_distance(feature_df)\n",
    "        \n",
    "    # add the avg_cluster_duration feature\n",
    "    if features_toggle['avg_cluster_duration']:\n",
    "        # check if y is None\n",
    "        if y is None:\n",
    "            raise Exception(\"y must be passed to generate_features if avg_cluster_duration is True\")\n",
    "        feature_df = add_avg_cluster_duration(feature_df, y)\n",
    "    \n",
    "    # return the feature dataframe\n",
    "    return feature_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
