{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from py_files.data_manager import get_X_y\n",
    "from py_files.features import distance, generate_features\n",
    "from config import data_path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import xgboost as xgb\n",
    "pd.options.display.float_format = '{:.6f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = get_X_y(force_clean=True)\n",
    "X, y = get_X_y()\n",
    "X = generate_features(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['pickup_datetime'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_model(X, y):\n",
    "    # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBRegressor(booster='gbtree', n_estimators=100, learning_rate=0.01, max_depth=10, alpha=0.1)\n",
    "\n",
    "    # Fit\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validate\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    return {'RMSE': mean_squared_error(y_test, y_pred, squared=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RMSE': 603.7398110737926}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, np.log(y), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "random_indices = np.random.choice(len(X_train), num_samples, replace=False)\n",
    "\n",
    "X_train = X_train.iloc[random_indices]\n",
    "y_train = y_train.iloc[random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the X and y, and add the features\n",
    "X, y = get_X_y(force_clean=True)\n",
    "feature_X = generate_features(X, y)\n",
    "\n",
    "# drop the pickup datetime feature since sklearn RandomForest does\n",
    "# not accept datetime columns\n",
    "feature_X = feature_X.drop(columns=['pickup_datetime'])\n",
    "\n",
    "# get the X and y for training\n",
    "X_train = feature_X.copy()\n",
    "y_train = y.copy()\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "# to speed up the grid search, we will use the first four instances\n",
    "# of each cluster-to-cluster pair of data points\n",
    "df = X_train.copy()\n",
    "df = df.sort_values(by='avg_cluster_duration')\n",
    "\n",
    "dfs = []\n",
    "\n",
    "sample_per_class = 3\n",
    "for _ in range(sample_per_class):\n",
    "    firsts = df['avg_cluster_duration'] != df['avg_cluster_duration'].shift(1)\n",
    "    dfs.append(df.loc[firsts].copy())\n",
    "    df = df.loc[~firsts].copy()\n",
    "\n",
    "# combine all of the reprsentative samples\n",
    "final_df = pd.concat(dfs, axis=0).sort_values('avg_cluster_duration')\n",
    "\n",
    "# shuffle the data so that it is no longer sorted by avg_cluster_duration\n",
    "X_train = final_df.copy().sample(frac=1)\n",
    "y_train = y_train.loc[X_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "Best parameters from grid search:  {'alpha': 0.1, 'booster': 'gbtree', 'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Create param grid\n",
    "param_grid = {\n",
    "    'booster': ['gbtree', 'dart'],\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'max_depth': [10, 20],\n",
    "    'alpha': [0.1, 0.5],\n",
    "}\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_root_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best params\n",
    "print('Best parameters from grid search: ', grid_search.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
