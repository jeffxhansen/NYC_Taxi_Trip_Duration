{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:** Jeff Hansen, Dylan Skinner, Jason Vasquez, Dallin Stewart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO:\n",
    "- add citations to similar problems like Google and Uber !\n",
    "\n",
    "We aim to tackle the challenge of predicting taxi ride durations in New York City based on starting and stopping coordinates. A 'taxi ride duration' refers to how long, in time, the taxi ride took. This research question is very relevant for urban commuters and transportation systems and incorporates the myriad factors influencing taxi ride times, including traffic patterns, congestion, time of day, and external variables such as events or weather conditions. The strengths of machine learning methods align well with the intricacies of this problem, and allow us to uncover more nuanced relationships within the data. Beyond individual convenience, the ability to predict taxi ride times carries practical implications for optimizing taxi fleet management, resource allocation, and enhancing overall traffic flow in the city. \n",
    "\n",
    "Kaggle published the dataset we are working on for a coding competition, so over one thousand other groups have investigated this research question. Successful teams performed feature engineering to create fields such as month, day, hour, day of the week, and used models such as Random Forest Regression, Extra Trees Regression, PCA, XGBoost, linear regression, and Light GBM. This dataset from Kaggle was procured from the NYC Taxi and Limousine Commission. The dataset is complete and does not contain missing data\n",
    "\n",
    "The original dataset also includes fields for the taxi driver, the number of passengers, and whether the trip time was recorded in real time. The second dataset contains weather information with timestamps, temperature, precipitation, cloud cover, and wind information at every hour. The NYC taxi cab dataset was published by NYC Taxi and Limousine Commission (TLC) in Big Query on Google Cloud Platform and is well documented and densely populated with over one million data points. The weather dataset is much more sparse and has a much larger time range than needed to match the NYC Taxi data. These datasets provide a good source for addressing our research questions because they extensively cover NYC taxi travel for a significant time period. In short, the data allows us to effectively identify significant features impacting taxi trip duration and develop a robust predictive model for accurate estimations.\n",
    "\n",
    "While our primary investigation is focused on determining taxi cab trip duration, we are also interested in several other questions. What dates, days of the week, and times of day are most busy? Where are the most popular destinations? Furthermore, the multifaceted exploration of temporal, spatial, and environmental influences on travel durations in NYC presents a well-rounded analysis to reveal comprehensive insights into travel behaviors. This poses questions such as how do environmental factors such as rain impact taxi popularity? Which pickup and drop off locations have the highest average time by distance? Our data is well suited for our research question in exploring and predicting taxi trip duration, and has a single, clear answer. A process of machine learning model development will help us go one step further and develop a robust predictive model to estimate and understand trip durations accurately. Our approach, seen below, is comprehensive and unique in several ways. We use K-means clustering to cluster the pick-up and drop-off locations together, we also used a grid search on each type of model to find optimal hyperparameters of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In our pursuit to enhance the predictive power of our model, we identified the necessity for additional features in our dataset. This realization led to the development of three distinct feature groups: datetime, distance, and weather. \n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "The taxi cab duraction dataset from Kaggle contained several outlier data that required to removal. For example, some trips lasted 1 second, and others lasted over 980 days. To prevent erroneous data, we removed all rows where <code>trip_duration</code> was in the $.005$ quantile or less than 60 seconds. The dataset also contained outliers in the pick up and drop off locations that were far outside New York City, which we fixed by removed any point outside of city limits. For the full plot implementation, see section <em>8. Data Cleaning </em>in the Appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime\n",
    "\n",
    "One of the most important features in our dataset is passenger pickup time, originally represented in a string in the format `YYYY-MM-DD HH:MM:SS`. We created multiple time features from this column including `pickup_month`, one-hot encoded `pickup_day`, `pickup_hour`, and `pickup_minute`. We also added other versions of these data points including one-hot encoded `pickup_period`, `pickup_hour_sin`, `pickup_hour_cos`, and `pickup_datetime_norm`. \n",
    "\n",
    "### Pickup Period\n",
    "\n",
    "The feature `pickup_period` captures the time of day when passengers were picked up in one of four periods: morning (6:00 AM to 12:00 PM), afternoon (12:00 PM to 6:00 PM), evening (6:00 PM to 12:00 AM), and night (12:00 AM to 6:00 AM). These divisions align intuitively with significant periods of the day for taxi services, such as morning rush hours and evening nightlife."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickup Period Sine/Cosine\n",
    "\n",
    "We applied a circular encoding to the hour of the day to account for the cyclical nature of the hours of the day. We created `pickup_hour_sin` and `pickup_hour_cos` features using sine and cosine transformations, that avoid discontinuities (such as the start and end of a day).\n",
    "\n",
    "\\begin{align}\n",
    "    \\begin{split}\n",
    "        \\text{hour\\_sin} = \\sin \\left( \\frac{2 \\pi \\cdot \\text{pickup\\_hour}}{24} \\right) \\quad\n",
    "        \\text{hour\\_cos} = \\cos \\left( \\frac{2 \\pi \\cdot \\text{pickup\\_hour}}{24} \\right).\n",
    "    \\end{split}\n",
    "\\end{align}\n",
    "\n",
    "### Pickup Datetime Norm\n",
    "\n",
    "The final feature we created in the datetime feature grouping was `pickup_datetime_norm` to represent the normalized pickup datetime. This feature converts the pickup datetime from nanoseconds to seconds, then scaled the value by the maximum to place all the values between 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance\n",
    "\n",
    "We created two features that estimate distance between pickup and drop off locations: the Manhattan distance <!--, the shortest path along New York roads according to Dijkstra's algorithm, -->and the average distances between local coordinate clusters. \n",
    "\n",
    "### Manhattan Distance\n",
    "\n",
    "We include the Manhattan Distance feature because of its grid based metric. Many of the streets of New York are laid out in a grid-like fashion, so this metric can better approximate road distances than the Euclidean distance. The Manhattan distance is also more simple and interpretable, since it computes the sum of the absolute values of the differences between the $x$ and $y$ coordinates of the two points. We calculated the Manhattan distance by first converting the pickup and dropoff coordinate points into radians and using the following formula:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Manhattan Distance} = R \\cdot \\left( \\left| \\text{pickup\\_latitude} - \\text{dropoff\\_latitude} \\right| + \\left| \\text{pickup\\_longitude} - \\text{dropoff\\_longitude} \\right| \\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $R$ is the radius of the Earth in kilometers (6371 km). \n",
    "\n",
    "<!-- <h3>2.1.1 Computing Distance with Dijkstra's Algorithm</h3>\n",
    "\n",
    "With over 1.4 million data points over a relatively small geography, the pick up and drop off locations provide an effective proxy for the actual road map of New York. We can use this information to estimate the distance between the two points using a modified version of Dijkstra's algorithm. We chose to use Dijkstra's algorithm because it is a well known and well documented algorithm for finding the shortest path between two points in a graph. We modified the algorithm to find the shortest path between two points in a graph, where the edges of the graph are the roads in New York City with weights equal to their geographic length, and the nodes are the pick up and drop off locations. \n",
    "\n",
    "We can reduce the size of the graph by removing nodes that are too close together, removing the few hundred nodes that lie well outside the metropolitan area, and cropping the graph for each distance calculation to only include nodes that lie roughly between the pick up and drop off locations. This reduces the size of the graph from over 1.4 million nodes to a few thousand nodes. We can then use Dijkstra's algorithm to find the shortest path between the pick up and drop off locations. The resulting cost of the shortest path is an estimate of the actual driving distance required to travel between the two points, which is one of the strongest predictors of taxi ride duration. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Clustering Average Duration\n",
    "\n",
    "Along with incorporating distance metrics and weather information, we also added a duration feature that acts as an initial estimate for the model's actual trip duration prediction. To do this, we fit a pickup and dropoff KMeans clustering model with 200 clusters. Images of these are shown in section `4. Data Visualization`. We then labeled each pickup location and drop off location in the data with its respective cluster label. By grouping the data by these cluster pairs, we computed the average trip_duration between each cluster pair and merged this onto the original dataframe. See section <em>6. KMeans Clustering</em> in Appendix for the full code implementation. This is helpful as it added an additional feature that gives the model a baseline for what duration to expect based on location, KMeans allows us to segment the data in a way that can group outliers, or large distances locations, together which will improve accuracy and keep like locations together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather\n",
    "\n",
    "When considering potential features to add to our dataset, accounting for the effect of local weather on taxi ride tme was one of the most obvious additions to include. For example, if it is raining or snowing, there will be more traffic on the roads, and more people who would normally walk would prefer a taxi. A dataset created by Kaggle user [@Aadam](https://www.kaggle.com/aadimator) contains a miriad of weather data for New York City between 2016 and 2022 on an hourly basis. This dataset includes features such as temperature (in Celcius), precipition (in mm), cloud cover (low, mid, high, and total), wind speed (in km/h), and wind direction. We decided to use temperature, precipitation, and total cloud cover as features in our dataset with a simple join on the pickup datetime, rounded to the nearest whole hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "As seen is section 2, we created several new features in addition to those already in the dataset. We also consider which features are unnecessary or unhelpful. \n",
    "\n",
    "## $L^1$ Regularization\n",
    "\n",
    "To figure out which of our features is most important, we utilized $L^1$ regularization since $L^1$ regularization naturally sets unneeded feature coefficents to 0, and typically out performs step-wise feature removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Alpha\t  : 1.0806\n",
      "Optimal BIC  \t  : 18056884.5229\n",
      "Lasso Coefficients: [ 0.    , 0.     , 0.    ,  0.    ,  0.    ,  0.    ,  0.    , 0.    , 0.    , 0.    ,\n",
      "\t\t      0.    , 0.     , 0.    ,  0.0169,  0.    ,  0.    ,  0.    , 0.    , 0.    , 0.    ,\n",
      "\t\t     -0.0345, -0.1089, 0.    ,  0.0147,  0.0043]\n",
      "Important Features: ['pickup_minute', 'distance_km', 'temperature_2m (°C)', 'cloudcover (%)', 'avg_cluster_duration']\n"
     ]
    }
   ],
   "source": [
    "# lasso_feature_selection performs feature selection using the LassoLarsIC method\n",
    "lasso_feature_selection(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Visualization\n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "Before visualizing the data, we first want to do basic data exploration, especially after adding the features mentioned above.\n",
    "\n",
    "For this project, we built the function `get_X_y()` that performs basic feature engineering, and returns two Pandas dataframes, $X$ and $y$. Additionally, we built the function `generate_features()` that performs more advanced feature engineering, returning again two Pandas dataframes, $X$ and $y$. Below we have a table that represents a row from our final feature_X dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1441615, 27), (1441615,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = get_X_y(force_clean=True)    # All feature engineering is done in the function get_X_y, found in appendix\n",
    "feature_X = generate_features(X, y) # generates features adds more features to the data, including weather\n",
    "feature_X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Value | Feature | Value | Feature | Value |\n",
    "| ------- | ----- | ------- | ----- | ------- | ----- |\n",
    "| vendor_id | 1 | pickup_datetime | 2016-03-14 17:24:55 | passenger_count | 1 |\n",
    "| pickup_longitude | -73.98 | pickup_latitude | 40.77 | dropoff_longitude | -73.96 |\n",
    "| dropoff_latitude | 40.77 | pickup_month | 3 | pickup_day_Monday | 1 |\n",
    "| pickup_day_Saturday | 0 | pickup_day_Sunday | 0 | pickup_day_Thursday | 0 |\n",
    "| pickup_day_Tuesday | 0 | pickup_day_Wednesday | 0 | pickup_hour | 17 |\n",
    "| pickup_minute | 24 | pickup_period_morning | 0 | pickup_period_afternoon | 1 |\n",
    "| pickup_period_evening | 0 | pickup_hour_sin | -0.97 | pickup_hour_cos | -0.26 |\n",
    "| pickup_datetime_norm | 0.41 | distance_km | 2.21 | temperature_2m (°C) | 6.40 |\n",
    "| precipitation (mm) | 0.20 | cloudcover (%) | 100.00 | avg_cluster_duration | 814.73 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "TODO:\n",
    "- make text in graphs bigger\n",
    "- make more clear conclusions about the data and why we have and can conclude these things\n",
    "\n",
    "In the following figure, the top two graphs visualize the pickup and dropoff locations overlaid over a map of NYC. The bottom two graphs shows the pickup and dropoff locations clustered into groups using K-means clustering. The pickup locations are more heavily clustered around downtown (Manhattan), while the dropoff locations are more evenly distributed throughout the city. For the full plot implementation, see section <em>7. Visualization </em>in the Appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/pickup_locations.png\" alt=\"pickup_locations\", width=400, height=300></td>\n",
    "        <td><img src=\"images/dropoff_locations.png\" alt=\"dropoff_locations\", width=400, height=300></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/kmeans_200_pickup.png\" alt=\"kmeans_200_pickup\", width=400, height=300></td>\n",
    "        <td><img src=\"images/kmeans_200_dropoff.png\" alt=\"kmeans_200_dropoff\", width=400, height=300></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "![Pickup Locations](images/pickup_locations.png)\n",
    "![Dropoff Locations](images/dropoff_locations.png)\n",
    "![KMeans 200 Pickup](images/kmeans_200_pickup.png)\n",
    "![KMeans 200 Dropoff](images/kmeans_200_dropoff.png) -->\n",
    "\n",
    "|  |  | \n",
    "| --- | --- |\n",
    "| ![Pickup Locations](images/pickup_locations.png) | ![Dropoff Locations](images/dropoff_locations.png) |\n",
    "| | |\n",
    "\n",
    "|  | |\n",
    "| --- | --- |\n",
    "|  ![KMeans 200 Pickup](images/kmeans_200_pickup.png) | ![KMeans 200 Dropoff](images/kmeans_200_dropoff.png) |\n",
    "|  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also learn about the factors that influence a New Yorker's decision to take a taxi from the data. For example, in the figure below, the graph on the left displays the most popular times of day to hail a taxi, which peeks around 6:00 PM. We also see that poor weather encourages more people to take a taxi during the day when people are more likely to be returning from their daily activities, but less likely to choose to go out at night in the first place. The graph on the right shows the most popular days of the week for taxis, which peeks on Friday and Saturday and during the evenings of the weekdays. Brighter colors indicate more taxi rides, and darker colors indicate fewer taxi rides. For the full plot implementation, see section <em>7. Visualization </em>in the Appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/pickup_rain_freq.png\" alt=\"pickup_rain_freq\", width=400, height=300></td>\n",
    "        <td><img src=\"images/day_hour.png\" alt=\"day_hour\", width=600, height=300></td>\n",
    "    </tr>\n",
    "</table> -->\n",
    "\n",
    "|  | |\n",
    "| --- | --- |\n",
    "|  ![Pickup Rain Frequency](images/pickup_rain_freq.png) | ![Day Hour Heatmap](images/day_hour.png) |\n",
    "|  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Results\n",
    "\n",
    "TODO:\n",
    "- talk about how we prevent overfitting\n",
    "- visualize the results of our models against each other\n",
    "\n",
    "Our primary research question is to predict the duration of a taxi ride in New York City. To achieve this goal, we implemented a variety of machine learning models, including Lasso regression, random forest regression, XGBoost, and LightGBM. We explain our implementation, training, optimization, and results for each model below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After iteratively designing features, we performed hyperparameter grid searches for the following models and they took this much time to run\n",
    "\n",
    "| Model | Time | Function Call |\n",
    "| ----- | ---- | ------------- |\n",
    "| Light GBM | 45min | `lightgbm_hyperparameter_search(X, y)` |\n",
    "| Lasso Regression | <1min | `lasso_regression_model(optimal_alpha=1.0806)` |\n",
    "| XGBoost | 9hr | `xgboost_hyperparameter_search(X, y)` |\n",
    "| Random Forest | 6hr | `random_forest_gridsearch()` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these were our final hyperparameter choices and the best RMSE\n",
    "\n",
    "\n",
    "| Parameter        | Lasso Params | LightGBM Params | LightGBM_large Params | RF Params | XGBoost Params |\n",
    "|------------------|--------------|-----------------|-----------------------|-----------|----------------|\n",
    "| boosting_type    | --           | gbdt            | gbdt                  | --        | gbtree         |\n",
    "| learning_rate    | --           | 0.01            | 0.01                  | --        | 0.01           |\n",
    "| max_depth        | --           | 20              | 50                    | 3         | 10             |\n",
    "| n_estimators     | --           | 100             | 100000                | 200       | 100            |\n",
    "| num_leaves       | --           | 30              | 500                   | --        | --             |\n",
    "| reg_alpha        | 1.0806       | 0.1             | 0.1                   | --        | 0.1            |\n",
    "| reg_lambda       | --           | 0.5             | 0.5                   | --        | --             |\n",
    "| max_features     | --           | --              | --                    | log2      | --             |\n",
    "| min_samples_leaf | --           | --              | --                    | 2         | --             |\n",
    "| Best RMSE        | 606.9680     | 606.9699        | 622.4705              | 605.1684  | 603.7398       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | |\n",
    "| --- | --- |\n",
    "|  ![Actual vs. Prediction Distribution](images/actual_vs_predicted.png) | ![Duration per Distance](images/actual_vs_predicted_lgb_large_large.png) \n",
    "|  | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- explain more clearly why our model did not work\n",
    "- tie back to original question to transition to conclusion\n",
    "- talk more about models and do more visualization here\n",
    "- explain more indepth why the graph on the right is less overfit\n",
    "\n",
    "The plot on the left demonstrates the predicted distribution of a LightGBM model with only 100 estimators, 30 leaves, and a max_depth of 50. The plot on the right demonstrates a similar LightGBM model but with 100,000 estimators, 500 leaves and a max_depth of 50. Interestingly, as we increase the compute abilitity of the model, the predictive distribution better matches the overall distribution; however, this model has a slightly worse RMSE. This evidences that the model is less overfit. \n",
    "\n",
    "Also note that both models tend to predict the mean. This implies that the feature space is not large enough for the model to create a more accurate approximation of the distribution. If we had more compute resources and time, we could raise model parameters to higher values and create more features. This would allow the model to better fit the actual distribution of trip durations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  |\n",
    "| --- |\n",
    "|  ![Duration per Distance](images/largest_duration_per_distance_resize.png)|\n",
    "|  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we depict which cluster pairs on average take the longest time to travel between them per distance. Interestingly, some of the cluster paths cross major roadways. This implies that there should be more support roads that cross under or over the major highways. Also, a majority of the largest duration trips per distance occur in Manhattan. This demonstrates that the public transit system could be improved to optimize taxi traffic use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethical Implications\n",
    "\n",
    "Our research involves analyzing a large dataset created by tracking some of the life-style patterns of real people living in New York during 2016, raising concerns about privacy and responsible data usage for individual behaviors, locations, and travel patterns. Our dataset and model protect this data by excluding all personally identifiable information to ensure that only aggregate information can be meaningful, and the patterns of individuals remain indiscernible.\n",
    "\n",
    "The risk of misinterpretation or misuse of our predictive models is very real. Users could misunderstand predictions, leading to inappropriate decision-making. Our predictive model may contain and inadvertently perpetuate biases that we are unaware of, such as inappropriate associations with certain neighborhoods and taxis. Furthermore, users might misunderstand the predictive and uncertain nature of the model and treat its estimates as certainties. For instance, if taxi companies or transportation authorities were to make decisions solely based on the model's predictions without considering broader traffic management strategies, it could inadvertently lead to concentrated traffic, worsening congestion in certain areas. If our model were deployed in conjunction with algorithms influencing taxi availability, the system might inadvertently create self-fulfilling feedback loops, disproportionately affecting certain areas or demographics.\n",
    "\n",
    "To address these issues, clear communication about the model's limitations, potential biases, and intended use is crucial. Providing educational resources, user-friendly interfaces, adequate documentation, and implementing fairness-aware algorithms can contribute to responsible and ethical deployment. Regular assessments, periodic audits, and interventions are necessary to avoid reinforcing existing biases. We have also considered ethical responsibilities such as the responsible disclosure of findings, ensuring the public benefits from the research, and avoiding any unintentional harm. Active engagement with potential stakeholders and the community can further help address concerns and foster ethical practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python imports\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Native imports\n",
    "from py_files.features import generate_features\n",
    "from py_files.data_manager import get_X_y, get_nyc_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lasso Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_feature_selection(X, y):\n",
    "    \"\"\"\n",
    "    Performs feature selection using the LassoLarsIC method\n",
    "\n",
    "    Parameters\n",
    "    - X (dataframe): dataframe of input features\n",
    "    - y (series): series of target values\n",
    "\n",
    "    Returns:\n",
    "    - (dict): dictionary of results (optimal alpha, optimal BIC, lasso coefficients, important features)\n",
    "    \"\"\"\n",
    "    lasso_lars_ic = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC(criterion=\"bic\", normalize=False)).fit(X, y)\n",
    "\n",
    "    results = pd.DataFrame(\n",
    "        {\n",
    "            \"alphas\": lasso_lars_ic[-1].alphas_,\n",
    "            \"BIC criterion\": lasso_lars_ic[-1].criterion_,\n",
    "        }\n",
    "    ).set_index(\"alphas\")\n",
    "\n",
    "    optimal_alpha = results[results['BIC criterion'] == results['BIC criterion'].min()].index\n",
    "\n",
    "    # Train a Lasso model with the optimal alpha for feature selection\n",
    "    lasso = linear_model.Lasso(alpha=optimal_alpha)\n",
    "    lasso.fit(X, y)\n",
    "\n",
    "    return {'Optimal Alpha': optimal_alpha.values[0], 'Optimal BIC': results.loc[optimal_alpha].values[0].tolist()[0],\n",
    "            'Lasso Coeffs': lasso.coef_.round(4), 'Important Features': X.columns[lasso.coef_ != 0].values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression_model(optimal_alpha):\n",
    "    \"\"\"\n",
    "    This function takes in the optimal alpha from the Lasso Lars IC Feature Selection and trains a Lasso Regression\n",
    "    model on the important features.\n",
    "\n",
    "    Parametes:\n",
    "    - optimal_alpha (float): The optimal alpha from the Lasso Lars IC Feature Selection\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with the RMSE of the model\n",
    "    \"\"\"\n",
    "    # Important features selected from Lasso Lars IC Feature Selection\n",
    "    important_features = ['pickup_minute', 'distance_km', 'temperature_2m (°C)', 'cloudcover (%)', 'avg_cluster_duration']\n",
    "\n",
    "    # Create dataframe of important features\n",
    "    X2 = X[important_features]\n",
    "\n",
    "    # Get test train split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and fit the model.\n",
    "    model = linear_model.Lasso(alpha=optimal_alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test data and compute RMSE\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {'RMSE': mean_squared_error(y_test, y_pred, squared=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LightGBM Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_hyperparameter_search(X, y):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter search for LightGBM model\n",
    "\n",
    "    Parameters:\n",
    "    - X (dataframe): dataframe of input features\n",
    "    - y (dataframe): dataframe of target variables\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary of best parameters and best RMSE\n",
    "    \"\"\"\n",
    "    # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Create param grid\n",
    "    param_grid = {\n",
    "        'boosting_type': ['gbdt', 'dart'],\n",
    "        'num_leaves': [30, 40],\n",
    "        'learning_rate': [0.01, 0.05],\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20],\n",
    "        'reg_alpha': [0.1, 0.5],\n",
    "        'reg_lambda': [0.1, 0.5],\n",
    "    }\n",
    "\n",
    "    # LightGBM\n",
    "    lgb_train = lgb.LGBMRegressor()\n",
    "\n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(estimator=lgb_train, param_grid=param_grid, cv=3, scoring='neg_root_mean_squared_error', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Validate\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    return {'Best parameters from grid search': grid_search.best_params_, 'Best RMSE': mean_squared_error(y_test, y_pred, squared=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. XGBoost Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_hyperparameter_search(X, y):\n",
    "    \"\"\"\n",
    "    Performs a grid search on the XGBoost model\n",
    "    \n",
    "    Parameters:\n",
    "    - X (DataFrame): The input features\n",
    "    - y (Series): The target variable\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    param_grid = {\n",
    "        'booster': ['gbtree', 'dart'],\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.05],\n",
    "        'max_depth': [10, 20],\n",
    "        'alpha': [0.1, 0.5],\n",
    "    }\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_root_mean_squared_error', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best params\n",
    "    print('Best parameters from grid search: ', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_model(X, y):\n",
    "    \"\"\"\n",
    "    XGBoost model with optimal hyperparameters\n",
    "    \n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary of RMSE results\n",
    "    \"\"\"\n",
    "    # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBRegressor(booster='gbtree', n_estimators=100, learning_rate=0.01, max_depth=10, alpha=0.1)\n",
    "\n",
    "    # Fit\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validate\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    return {'RMSE': mean_squared_error(y_test, y_pred, squared=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Random Forest Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_gridsearch():\n",
    "    \"\"\"\n",
    "    Performs a hyperparameter gridsearch with cross validation\n",
    "    to find the optimal parameters for a RandomForestRegressor\n",
    "    \n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "        \n",
    "    # get the X and y, and add the features\n",
    "    X, y = get_X_y(force_clean=True)\n",
    "    feature_X = generate_features(X, y)\n",
    "\n",
    "    # drop the pickup datetime feature since sklearn RandomForest does\n",
    "    # not accept datetime columns\n",
    "    feature_X = feature_X.drop(columns=['pickup_datetime'])\n",
    "\n",
    "    # get the X and y for training\n",
    "    X_train = feature_X.copy()\n",
    "    y_train = y.copy()\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "    # to speed up the grid search, we will use the first four instances\n",
    "    # of each cluster-to-cluster pair of data points\n",
    "    df = X_train.copy()\n",
    "    df = df.sort_values(by='avg_cluster_duration')\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    sample_per_class = 4\n",
    "    for _ in range(sample_per_class):\n",
    "        firsts = df['avg_cluster_duration'] != df['avg_cluster_duration'].shift(1)\n",
    "        dfs.append(df.loc[firsts].copy())\n",
    "        df = df.loc[~firsts].copy()\n",
    "\n",
    "    # combine all of the reprsentative samples\n",
    "    final_df = pd.concat(dfs, axis=0).sort_values('avg_cluster_duration')\n",
    "\n",
    "    # shuffle the data so that it is no longer sorted by avg_cluster_duration\n",
    "    X_train = final_df.copy().sample(frac=1)\n",
    "    y_train = y_train.loc[X_train.index]\n",
    "\n",
    "    X_train = X_train.values.astype(np.float32)\n",
    "    y_train = y_train.values.astype(np.float32)\n",
    "\n",
    "    # perform the RandomForest gridsearch to find the best\n",
    "    # hyperparameters\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 400, 800, 1000],\n",
    "        'max_depth': [None, 3, 5, 10, 20],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'min_samples_leaf': [1, 2, 3, 5],\n",
    "    }\n",
    "    rf = RandomForestRegressor(warm_start=False)\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, verbose=3, n_jobs=-2, cv=4).fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # save the grid_search_model for future loading\n",
    "    with open(\"models/rf_grid_search.pkl\", \"wb\") as f:\n",
    "        pickle.dump(grid_search, f)\n",
    "        \n",
    "    # print the best parameters\n",
    "    print(\"Best parameters RandomForest:\", best_params)\n",
    "    \n",
    "    # train a model and compute the RMSE on the test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    rf = RandomForestRegressor(**best_params)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    rf_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    print(\"RandomForest RMSE:\", rf_rmse)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_pickup_dropoff_model(df, n_clusters=200):\n",
    "    \"\"\"\n",
    "    Fits KMeans models for pickup and dropoff locations, labels each location by its cluster,\n",
    "    computes the average duration between each cluster, and merges it onto the original dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing pickup and dropoff locations.\n",
    "    - n_clusters (int): The number of clusters for KMeans.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The modified DataFrame with cluster labels and average cluster duration.\n",
    "    \"\"\"\n",
    "    # fit the kmeans models and label each pickup and dropoff location by its cluster\n",
    "    kmeans_pickup = (KMeans(n_clusters=n_clusters)\n",
    "        .fit(df.loc[:, ['pickup_longitude', 'pickup_latitude']].values))\n",
    "    kmeans_dropoff = (KMeans(n_clusters=n_clusters)\n",
    "        .fit(df.loc[:, ['dropoff_longitude', 'dropoff_latitude']].values))\n",
    "    df['pickup_cluster'] = kmeans_pickup.predict(df[['pickup_longitude', 'pickup_latitude']].values)\n",
    "    df['dropoff_cluster'] = kmeans_dropoff.predict(df[['dropoff_longitude', 'dropoff_latitude']].values)\n",
    "\n",
    "    # compute the average duration between each cluster and merge this onto the original dataframe\n",
    "    group_durations = (df\n",
    "        .groupby(['pickup_cluster', 'dropoff_cluster'])['trip_duration']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'trip_duration': 'avg_cluster_duration'}))\n",
    "    df = pd.merge(\n",
    "        left=df, right=group_durations, how='left',\n",
    "        left_on=['pickup_cluster', 'dropoff_cluster'], right_on=['pickup_cluster', 'dropoff_cluster'])\n",
    "\n",
    "    # fill the missing values with the mean of the average duration from cluster to cluster\n",
    "    df['avg_cluster_duration'] = df['avg_cluster_duration'].fillna(df['avg_cluster_duration'].mean())\n",
    "    df.drop(columns=['pickup_200_cluster', 'dropoff_200_cluster', 'trip_duration'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 KMeans Clustering Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants/parameters for this code cell\n",
    "SHOW_PLOTS = True\n",
    "LOAD_SAVED_KMEANS_MODELS = True\n",
    "\n",
    "# load in the cleaned training data and the NYC geopandas dataframe\n",
    "# with all of the NYC streets\n",
    "X, y = get_X_y(force_clean=True)\n",
    "nyc_gdf = get_nyc_gdf()\n",
    "\n",
    "\n",
    "#########################\n",
    "# PLOT PICKUP LOCATIONS #\n",
    "#########################\n",
    "def plot_pickup_locations(X):\n",
    "    \"\"\"\n",
    "    Plots the NYC streets and pickup locations as a scatter plot on top of the streets.\n",
    "\n",
    "    Parameters:\n",
    "    - X (DataFrame): The DataFrame containing pickup locations.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # plot the nyc streets\n",
    "    plt.gcf().set_dpi(500)\n",
    "    nyc_gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "    # plot the pickup locations as a scatter plot on top of the nyc streets\n",
    "    plt.scatter(X['pickup_longitude'], X['pickup_latitude'], c='red', alpha=0.75, s=0.1, label=\"Pickup Locations\")\n",
    "    leg = plt.legend(loc='upper left')\n",
    "    for lh in leg.legend_handles: \n",
    "        lh.set_alpha(1)\n",
    "    plt.title(\"Pickup Locations\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    # save the plot\n",
    "    plt.savefig(\"images/pickup_locations_save.png\")\n",
    "    plt.show() if SHOW_PLOTS else plt.clf()\n",
    "\n",
    "\n",
    "##########################\n",
    "# PLOT DROPOFF LOCATIONS #\n",
    "##########################\n",
    "def plot_dropoff_locations(X):\n",
    "    \"\"\"\n",
    "    Plots the NYC streets and dropoff locations as a scatter plot on top of the streets.\n",
    "\n",
    "    Parameters:\n",
    "    - X (DataFrame): The DataFrame containing dropoff locations.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # plot the nyc streets\n",
    "    plt.gcf().set_dpi(500)\n",
    "    nyc_gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "    # plot the dropoff locations as a scatter plot on top of the nyc streets\n",
    "    plt.scatter(X['dropoff_longitude'], X['dropoff_latitude'], c='green', alpha=0.75, s=0.1, label=\"Dropoff Locations\")\n",
    "    leg = plt.legend(loc='upper left')\n",
    "    for lh in leg.legend_handles: \n",
    "        lh.set_alpha(1)\n",
    "    plt.title(\"Dropoff Locations\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    # save the plot\n",
    "    plt.savefig(\"images/dropoff_locations_save.png\")\n",
    "    plt.show() if SHOW_PLOTS else plt.clf()\n",
    "\n",
    "\n",
    "#####################\n",
    "# KMEANS CLUSTERING #\n",
    "#####################\n",
    "def kmeans_pickup_dropoff_predict(df, n_clusters=200):\n",
    "    \"\"\"\n",
    "    Applies KMeans clustering to predict pickup and dropoff locations, or loads pre-trained models if available.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing pickup and dropoff locations.\n",
    "    - n_clusters (int): The number of clusters for KMeans. Default is 200.\n",
    "\n",
    "    Returns:\n",
    "    - df (DataFrame): The DataFrame with predicted clusters for pickup and dropoff locations.\n",
    "    - pickup_200_centers (array): The cluster centers for pickup locations.\n",
    "    - dropoff_200_centers (array): The cluster centers for dropoff locations.\n",
    "    \"\"\"\n",
    "    df = deepcopy(X)\n",
    "\n",
    "    # load kmeans_pickup and kmeans_dropoff from the models folder using pickle\n",
    "    if LOAD_SAVED_KMEANS_MODELS:\n",
    "        with open(\"models/kmeans_200_pickup.pkl\", \"rb\") as file:\n",
    "            kmeans_200_pickup = pickle.load(file)\n",
    "        with open(\"models/kmeans_200_dropoff.pkl\", \"rb\") as file:\n",
    "            kmeans_200_dropoff = pickle.load(file)\n",
    "        \n",
    "            \n",
    "    # fit kmeans_pickup and kmeans_dropoff with 200 clusters\n",
    "    else:\n",
    "        n_clusters = 200\n",
    "        kmeans_pickup = (KMeans(n_clusters=n_clusters)\n",
    "            .fit(df.loc[:, ['pickup_longitude', 'pickup_latitude']].values))\n",
    "        kmeans_dropoff = (KMeans(n_clusters=n_clusters)\n",
    "            .fit(df.loc[:, ['dropoff_longitude', 'dropoff_latitude']].values))\n",
    "        \n",
    "        # save the models to pickle files for loading later\n",
    "        with open(\"models/kmeans_200_pickup.pkl\", \"wb\") as file:\n",
    "            pickle.dump(kmeans_pickup, file)\n",
    "        with open(\"models/kmeans_200_dropoff.pkl\", \"wb\") as file:\n",
    "            pickle.dump(kmeans_dropoff, file)\n",
    "\n",
    "    # predict the clusters for each pickup and dropoff location\n",
    "    df['pickup_200_cluster'] = kmeans_200_pickup.predict(df[['pickup_longitude', 'pickup_latitude']].values)\n",
    "    df['dropoff_200_cluster'] = kmeans_200_dropoff.predict(df[['dropoff_longitude', 'dropoff_latitude']].values)\n",
    "\n",
    "    # get the centers\n",
    "    pickup_200_centers = kmeans_200_pickup.cluster_centers_\n",
    "    dropoff_200_centers = kmeans_200_dropoff.cluster_centers_\n",
    "    return df, pickup_200_centers, dropoff_200_centers\n",
    "\n",
    "\n",
    "#######################################\n",
    "# PLOT PICKUP LOCATIONS WITH CLUSTERS #\n",
    "#######################################\n",
    "def plot_cluster_pickup(df, pickup_200_centers):\n",
    "    \"\"\"\n",
    "    Plots KMeans clustering for pickup locations.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing pickup locations and their associated clusters.\n",
    "    - pickup_200_centers (array): The cluster centers for pickup locations.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # plot the nyc streets\n",
    "    plt.gcf().set_dpi(500)\n",
    "    nyc_gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "    # plot the cluster locations and the pickup locations color-coded\n",
    "    # to their associated cluster\n",
    "    plt.scatter(df['pickup_longitude'], df['pickup_latitude'], c=df['pickup_200_cluster'], cmap='magma', alpha=1.0, s=0.1, label=\"Pickup Locations\")\n",
    "    plt.scatter(pickup_200_centers[:, 0], pickup_200_centers[:, 1], c='red', alpha=1, s=10, label=\"Cluster Centers\")\n",
    "    leg = plt.legend(loc='upper left')\n",
    "    for lh in leg.legend_handles: \n",
    "        lh.set_alpha(1)\n",
    "    plt.title(\"200-KMeans Clustering for Pickup Locations\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    # save the plot\n",
    "    plt.savefig(\"images/kmeans_200_pickup_save.png\")\n",
    "    plt.show() if SHOW_PLOTS else plt.clf()\n",
    "\n",
    "\n",
    "########################################\n",
    "# PLOT DROPOFF LOCATIONS WITH CLUSTERS #\n",
    "########################################\n",
    "def plot_cluster_dropoff(df, dropoff_200_centers):\n",
    "    \"\"\"\n",
    "    Plots KMeans clustering for dropoff locations.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing dropoff locations and their associated clusters.\n",
    "    - dropoff_200_centers (array): The cluster centers for dropoff locations.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # plot the nyc streets\n",
    "    plt.gcf().set_dpi(500)\n",
    "    nyc_gdf.plot(linewidth=0.1, edgecolor='black', figsize=(12, 12), alpha=0.5, label=\"NYC Streets\")\n",
    "\n",
    "    # plot the cluster locations and the pickup locations color-coded\n",
    "    # to their associated cluster\n",
    "    plt.scatter(df['dropoff_longitude'], df['dropoff_latitude'], c=df['dropoff_200_cluster'], cmap='viridis', alpha=1.0, s=0.1, label=\"Dropoff Locations\")\n",
    "    plt.scatter(dropoff_200_centers[:, 0], dropoff_200_centers[:, 1], c='blue', alpha=1, s=10, label=\"Cluster Centers\")\n",
    "    leg = plt.legend(loc='upper left')\n",
    "    for lh in leg.legend_handles: \n",
    "        lh.set_alpha(1)\n",
    "    plt.title(\"200-KMeans Clustering for Dropoff Locations\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "\n",
    "    # save the plot\n",
    "    plt.savefig(\"images/kmeans_200_dropoff_save.png\")\n",
    "    plt.show() if SHOW_PLOTS else plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted_distributions(show=False):\n",
    "    \"\"\"\n",
    "    Plots histograms of actual and predicted trip durations on the test set and compares their distributions.\n",
    "    \n",
    "    Parameters:\n",
    "    - show (bool): If True, displays the plot; if False, saves the plot as 'images/actual_vs_predicted.png'.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    # load in the actual and predicted trip durations\n",
    "    X, y = get_X_y()\n",
    "    feature_X = generate_features(X, y)\n",
    "    feature_X = feature_X.drop(columns=['pickup_datetime'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # plot a histogram of the actual trip duration distribution in the test set\n",
    "    # and compute the mean\n",
    "    counts, bins, patches = plt.hist(y_test, label='Actual Trip Durations')\n",
    "    actual_mean = np.mean(y_test)\n",
    "\n",
    "    # plot a histogram of the predicted trip duration distribution on the test set\n",
    "    counts_pred, bins_pred, pathces_pred = plt.hist(y_pred, label='Predicted Trip Durations', color='orange')\n",
    "\n",
    "    # plot a verticle line at the actual mean of the distribution\n",
    "    top = max(np.max(counts), np.max(counts_pred))\n",
    "    plt.vlines([actual_mean], 0, top, color='black', linestyles='dashed', label='Actual Mean')\n",
    "\n",
    "    # set other plot parameters and show the plot\n",
    "    plt.legend()\n",
    "    plt.title(\"Actual and Estimated Distribution of Trip Durations\")\n",
    "    plt.xlabel(\"Trip Duration (seconds)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig('images/actual_vs_predicted.png')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this py file contains all of the data loading, cleaning, and saving logic.\n",
    "The methods will automatically pull in a cached version of the dataframe\n",
    "unless force_clean=True. If force_clean=True, then the dataframe will be\n",
    "cleaned and saved to the data folder.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import geopandas as gpd\n",
    "from shapely.wkt import loads\n",
    "\n",
    "from config import (\n",
    "    data_path, cols_to_drop, SET_VENDOR_ID_TO_01,\n",
    "    PICKUP_TIME_TO_NORMALIZED_FLOAT\n",
    ")\n",
    "from py_files.helper_funcs import p\n",
    "\n",
    "\n",
    "def clean_data(df, df_name, verbose=False):\n",
    "    \"\"\"\n",
    "    Loads in the train.csv and test.csv and cleans them according\n",
    "    to the constants in config.py. Saves the cleaned dataframes as\n",
    "    train_clean.csv and test_clean.csv\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas dataframe): The dataframe to be cleaned\n",
    "    - df_name (str): The name of the dataframe, either 'train' or 'test'\n",
    "    - verbose (bool): If True, prints out the progress of the cleaning\n",
    "\n",
    "    Returns:\n",
    "    - df_clean (pandas dataframe): The cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # only keep the relevant columns based on the config\n",
    "    p(\"dropping columns\") if verbose else None\n",
    "    curr_cols_to_drop = [c for c in df.columns if c in cols_to_drop]\n",
    "    df_clean = df.drop(columns=curr_cols_to_drop)\n",
    "    p() if verbose else None\n",
    "\n",
    "    # setting vendor_id to a 0 or 1 instead of 1 and 2\n",
    "    if SET_VENDOR_ID_TO_01:\n",
    "        p(\"setting vendor_id to 0 or 1\") if verbose else None\n",
    "        df_clean['vendor_id'] = df_clean['vendor_id'] - 1\n",
    "        p() if verbose else None\n",
    "\n",
    "    # Drop rows with trip duration < 60 seconds\n",
    "    p(\"dropping rows with trip duration < 60 seconds\") if verbose else None\n",
    "    df_clean = df_clean[df_clean['trip_duration'] >= 60]\n",
    "\n",
    "    # Drop rows with outlier locations\n",
    "    p(\"dropping rows with outlier locations\") if verbose else None\n",
    "    json_file_path = './misc/lat_long_bounds.json'\n",
    "    # Read in coordinates\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        # Load the JSON data from the file\n",
    "        coords = json.load(json_file)\n",
    "    df_clean = df_clean[(df_clean['pickup_latitude'] >= coords['lat']['min']) & (\n",
    "        df_clean['pickup_latitude'] <= coords['lat']['max'])]\n",
    "    df_clean = df_clean[(df_clean['pickup_longitude'] >= coords['lon']['min']) & (\n",
    "        df_clean['pickup_longitude'] <= coords['lon']['max'])]\n",
    "    df_clean = df_clean[(df_clean['dropoff_latitude'] >= coords['lat']['min']) & (\n",
    "        df_clean['dropoff_latitude'] <= coords['lat']['max'])]\n",
    "    df_clean = df_clean[(df_clean['dropoff_longitude'] >= coords['lon']['min']) & (\n",
    "        df_clean['dropoff_longitude'] <= coords['lon']['max'])]\n",
    "\n",
    "    # Keep only <99.5% of trip duration\n",
    "    p(\"dropping rows with trip duration > 99.5%\") if verbose else None\n",
    "    df_clean = df_clean[df_clean['trip_duration'] <=\n",
    "                        df_clean['trip_duration'].quantile(0.995)]\n",
    "\n",
    "    # Split apart pickup_datetime\n",
    "    df_clean['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df_clean['pickup_month'] = df_clean['pickup_datetime'].dt.month\n",
    "    df_clean['pickup_day'] = df_clean['pickup_datetime'].dt.day_name()\n",
    "    df_clean = pd.get_dummies(\n",
    "        df_clean, columns=['pickup_day'], drop_first=True)\n",
    "    df_clean['pickup_hour'] = df_clean['pickup_datetime'].dt.hour\n",
    "    df_clean['pickup_minute'] = df_clean['pickup_datetime'].dt.minute\n",
    "\n",
    "    # Create a pickup period.\n",
    "    df_clean['pickup_period'] = pd.cut(df_clean['pickup_hour'], bins=[\n",
    "                                       -1, 6, 12, 18, 24], labels=['night', 'morning', 'afternoon', 'evening'])\n",
    "\n",
    "    # Get dummies for the pickup period.\n",
    "    df_clean = pd.get_dummies(\n",
    "        df_clean, columns=['pickup_period'], drop_first=True)\n",
    "\n",
    "    # Add cyclic data.\n",
    "    df_clean['pickup_hour_sin'] = np.sin(\n",
    "        2 * np.pi * df_clean['pickup_hour'] / 24)\n",
    "    df_clean['pickup_hour_cos'] = np.cos(\n",
    "        2 * np.pi * df_clean['pickup_hour'] / 24)\n",
    "\n",
    "    # convert pickup and dropoff times to floats from 0 to 1\n",
    "    if PICKUP_TIME_TO_NORMALIZED_FLOAT:\n",
    "        df_clean['pickup_datetime_norm'] = pd.to_datetime(\n",
    "            df_clean['pickup_datetime']).view('int64') // 10**9\n",
    "        df_clean['pickup_datetime_norm'] = (df_clean['pickup_datetime_norm'] - df_clean['pickup_datetime_norm'].min()) / (\n",
    "            df_clean['pickup_datetime_norm'].max() - df_clean['pickup_datetime_norm'].min())\n",
    "    \n",
    "    # Drop the id column\n",
    "    df_clean = df_clean.drop(columns=['id'])\n",
    "\n",
    "    # save the cleaned dataframe\n",
    "    p(\"saving cleaned dataframe\") if verbose else None\n",
    "    df_clean.to_csv(f\"{data_path}/{df_name}_clean.csv\", index=False)\n",
    "    p() if verbose else None\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def get_train_data(force_clean=False):\n",
    "    \"\"\"\n",
    "    Either creates the cleaned train dataframe from the train.csv\n",
    "    or it loads it from the data folder\n",
    "\n",
    "    Parameters:\n",
    "    - force_clean (bool): If True, forces the data to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    - train (pandas dataframe): A dataframe of the train data\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f\"{data_path}/train_clean.csv\") or force_clean:\n",
    "        train = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "        return clean_data(train, 'train')\n",
    "    else:\n",
    "        return pd.read_csv(f\"{data_path}/train_clean.csv\")\n",
    "\n",
    "\n",
    "def get_X_y(return_np=False, force_clean=False):\n",
    "    \"\"\"\n",
    "    Returns the X and y dataframes from a dataframe\n",
    "\n",
    "    Parameters:\n",
    "    - return_np (bool): If True, returns numpy arrays instead of\n",
    "    - force_clean (bool): If True, forces the data to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    - X (pandas dataframe): A dataframe of the input data\n",
    "    - y (pandas dataframe): A dataframe of the label data\n",
    "    \"\"\"\n",
    "    df = get_train_data(force_clean=force_clean)\n",
    "    X = df.drop(columns=['trip_duration'])\n",
    "    y = df['trip_duration']\n",
    "\n",
    "    if return_np:\n",
    "        X, y = X.values, y.values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_test_data():\n",
    "    \"\"\"\n",
    "    Either creates the cleaned test dataframe from the test.csv\n",
    "    or it loads it from the data folder\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - test (pandas dataframe): A dataframe of the test data\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f\"{data_path}/test_clean.csv\"):\n",
    "        test = pd.read_csv(f\"{data_path}/test.csv\")\n",
    "        return clean_data(test, 'test')\n",
    "    else:\n",
    "        return pd.read_csv(f\"{data_path}/test_clean.csv\")\n",
    "\n",
    "\n",
    "def get_clean_weather():\n",
    "    \"\"\"\n",
    "    Loads in the NYC_Weather_2016_2022.csv and cleans it according\n",
    "    to the constants in config.py. Saves the cleaned dataframe as\n",
    "    weather_clean.csv\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - weather (pandas dataframe): A dataframe of the weather\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f\"{data_path}/weather_clean1.csv\"):\n",
    "        weather = pd.read_csv(f\"{data_path}/NYC_Weather_2016_2022.csv\")\n",
    "        weather = weather.dropna()\n",
    "        weather['time'] = pd.to_datetime(weather['time'])\n",
    "        weather = weather[weather['time'] <= '2016-07-01']\n",
    "        weather = weather.drop(columns=['rain (mm)',\n",
    "                                        'cloudcover_low (%)',\n",
    "                                        'cloudcover_mid (%)',\n",
    "                                        'cloudcover_high (%)',\n",
    "                                        'windspeed_10m (km/h)',\n",
    "                                        'winddirection_10m (°)'])\n",
    "        weather.to_csv(f\"{data_path}/weather_clean1.csv\", index=False)\n",
    "        return weather\n",
    "    else:\n",
    "        return pd.read_csv(f\"{data_path}/weather_clean1.csv\")\n",
    "\n",
    "\n",
    "def get_google_distance():\n",
    "    \"\"\"\n",
    "    Loads in the train_distance_matrix.csv and cleans it according\n",
    "    to the constants in config.py. Saves the cleaned dataframe as\n",
    "    google_distance_clean.csv\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - google_distance (pandas dataframe): A dataframe of the google\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f\"{data_path}/google_distance_clean.csv\"):\n",
    "        google_distance = pd.read_csv(f\"{data_path}/train_distance_matrix.csv\")\n",
    "\n",
    "        columns_to_keep = ['id', 'gc_distance', 'google_distance']\n",
    "        google_distance = google_distance[columns_to_keep]\n",
    "\n",
    "        google_distance.to_csv(\n",
    "            f\"{data_path}/google_distance_clean.csv\", index=False)\n",
    "        return google_distance\n",
    "    else:\n",
    "        return pd.read_csv(f\"{data_path}/google_distance_clean.csv\")\n",
    "    \n",
    "    \n",
    "def get_nyc_gdf():\n",
    "    \"\"\"\n",
    "    Loads in the NYC street centerline data and returns it as a\n",
    "    geopandas dataframe\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - gdf (geopandas dataframe): A geopandas dataframe of the NYC\n",
    "    \"\"\"\n",
    "    nyc_df = pd.read_csv(f\"{data_path}/Centerline.csv\")\n",
    "    nyc_df = nyc_df.loc[:, ['the_geom']]\n",
    "\n",
    "    # Convert the \"the_geom\" column to Shapely geometries\n",
    "    nyc_df['the_geom_geopandas'] = nyc_df['the_geom'].apply(loads)\n",
    "\n",
    "    # Create a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(nyc_df, geometry='the_geom_geopandas')\n",
    "    \n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this py file holds all of the logic behind the feature engineering.\n",
    "The generate_features function takes in an X and a y, and it adds\n",
    "feature columns based on the config.features_toggle\n",
    "\"\"\"\n",
    "\n",
    "from config import features_toggle\n",
    "from py_files.data_manager import get_clean_weather, get_google_distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "def distance(df):\n",
    "    \"\"\"\n",
    "    Calculate the Manhattan distance in kilometers between pickup and dropoff locations\n",
    "    and add it as a new column 'distance_km' to the DataFrame.\n",
    "\n",
    "    The Manhattan distance, also known as the L1 distance or taxicab distance, between two points\n",
    "    on the Earth's surface is calculated by finding the absolute differences between their respective\n",
    "    longitudes and latitudes and summing them up. This function computes the Manhattan distance\n",
    "    in kilometers between the pickup and dropoff locations in a DataFrame, assuming a constant\n",
    "    Earth radius of 6371 kilometers.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): A DataFrame containing pickup and dropoff coordinates with columns\n",
    "                           'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', and 'dropoff_latitude'.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with an additional 'distance_km' column representing the Manhattan\n",
    "                     distance in kilometers between pickup and dropoff locations.\n",
    "    \"\"\"\n",
    "    # Radius of the Earth in kilometers\n",
    "    earth_radius_km = 6371.0\n",
    "\n",
    "    # Get the pickup and dropoff coordinates\n",
    "    lon1 = df['pickup_longitude']\n",
    "    lat1 = df['pickup_latitude']\n",
    "    lon2 = df['dropoff_longitude']\n",
    "    lat2 = df['dropoff_latitude']\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "\n",
    "    # Calculate the differences in latitude and longitude\n",
    "    delta_lat = abs(lat1 - lat2)\n",
    "    delta_lon = abs(lon1 - lon2)\n",
    "\n",
    "    # Calculate the Manhattan distance in kilometers\n",
    "    df['distance_km'] = earth_radius_km * (delta_lat + delta_lon)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_weather_feature(df):\n",
    "    \"\"\"\n",
    "    Add weather-related features to a DataFrame by merging it with a weather dataset.\n",
    "\n",
    "    This function merges the input DataFrame with a weather dataset based on the rounded pickup time\n",
    "    and adds weather-related features to the DataFrame. It rounds the 'pickup_datetime' column to the\n",
    "    nearest hour to match the weather data's time resolution.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing pickup-related data.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with added weather-related features merged from the weather dataset.\n",
    "    \"\"\"\n",
    "    # Get weather data\n",
    "    weather = get_clean_weather()\n",
    "    \n",
    "    # Round the pickup time to the nearest hour (to merge with weather)\n",
    "    df['rounded_date'] = pd.to_datetime(df['pickup_datetime']).dt.round('H')\n",
    "    weather['time'] = pd.to_datetime(weather['time'])\n",
    "\n",
    "    # Merge with weather\n",
    "    df = df.merge(weather, left_on='rounded_date', right_on='time')\n",
    "\n",
    "    # Drop unnecessary columns and return the dataframe\n",
    "    df = df.drop(columns=['rounded_date', 'time'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_google_distance(df):\n",
    "    \"\"\"\n",
    "    Add Google Maps distance and duration features to a DataFrame by merging it with a Google Maps dataset.\n",
    "    \n",
    "    Parameters\n",
    "    - df (DataFrame): The input DataFrame containing pickup and dropoff coordinates.\n",
    "    \n",
    "    Returns\n",
    "    - df (DataFrame): The DataFrame with added Google Maps distance and duration features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the google distance\n",
    "    google_distance = get_google_distance()\n",
    "\n",
    "    # Merge with the dataframe (verify 1:1)\n",
    "    df = df.merge(google_distance, on='id', validate='1:1')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_avg_cluster_duration(df, y):\n",
    "    \"\"\"\n",
    "    Adds a column 'avg_cluster_duration' to the DataFrame representing the average duration \n",
    "    from cluster to cluster based on pickup and dropoff locations.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing features.\n",
    "    - y (array-like): The array of target values (trip durations).\n",
    "\n",
    "    Returns:\n",
    "    - df (DataFrame): The DataFrame with added 'avg_cluster_duration' column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['trip_duration'] = y\n",
    "    \n",
    "    # load kmeans_pickup and kmeans_dropoff from the models folder using pickle\n",
    "    with open(\"models/kmeans_200_pickup.pkl\", \"rb\") as file:\n",
    "        kmeans_200_pickup = pickle.load(file)\n",
    "    with open(\"models/kmeans_200_dropoff.pkl\", \"rb\") as file:\n",
    "        kmeans_200_dropoff = pickle.load(file)\n",
    "        \n",
    "    # predict the clusters for the pickup and dropoff locations using the kmeans_pickup and kmeans_dropoff\n",
    "    df['pickup_200_cluster'] = kmeans_200_pickup.predict(df[['pickup_longitude', 'pickup_latitude']].values)\n",
    "    df['dropoff_200_cluster'] = kmeans_200_dropoff.predict(df[['dropoff_longitude', 'dropoff_latitude']].values)\n",
    "\n",
    "    # get the centers\n",
    "    pickup_200_centers = kmeans_200_pickup.cluster_centers_\n",
    "    dropoff_200_centers = kmeans_200_dropoff.cluster_centers_\n",
    "\n",
    "    # compute the average duration from cluster to cluster\n",
    "    group_durations = (df\n",
    "        .groupby(['pickup_200_cluster', 'dropoff_200_cluster'])['trip_duration']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'trip_duration': 'avg_cluster_duration'}))\n",
    "\n",
    "    # merge the average duration from cluster to cluster with the main dataframe\n",
    "    df = pd.merge(\n",
    "        left=df, right=group_durations, how='left',\n",
    "        left_on=['pickup_200_cluster', 'dropoff_200_cluster'], right_on=['pickup_200_cluster', 'dropoff_200_cluster'])\n",
    "\n",
    "    # fill the missing values with the mean of the average duration from cluster to cluster\n",
    "    df['avg_cluster_duration'] = df['avg_cluster_duration'].fillna(df['avg_cluster_duration'].mean())\n",
    "    df.drop(columns=['pickup_200_cluster', 'dropoff_200_cluster', 'trip_duration'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_features(df, y=None):\n",
    "    \"\"\"\n",
    "    Generates additional features based on the specified feature toggles and appends them to the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing base features.\n",
    "    - y (array-like, optional): The array of target values. Required if 'avg_cluster_duration' feature is enabled.\n",
    "\n",
    "    Returns:\n",
    "    - feature_df (DataFrame): The DataFrame with added features.\n",
    "    \"\"\"\n",
    "    # append the features to the dataframe\n",
    "    feature_df = df\n",
    "\n",
    "    # add the distance feature\n",
    "    if features_toggle['distance']:\n",
    "        feature_df = distance(feature_df)\n",
    "    \n",
    "    # add the weather feature\n",
    "    if features_toggle['weather']:\n",
    "        feature_df = add_weather_feature(feature_df)\n",
    "\n",
    "    # add the google distance feature\n",
    "    if features_toggle['google_distance']:\n",
    "        feature_df = add_google_distance(feature_df)\n",
    "        \n",
    "    # add the avg_cluster_duration feature\n",
    "    if features_toggle['avg_cluster_duration']:\n",
    "        # check if y is None\n",
    "        if y is None:\n",
    "            raise Exception(\"y must be passed to generate_features if avg_cluster_duration is True\")\n",
    "        feature_df = add_avg_cluster_duration(feature_df, y)\n",
    "    \n",
    "    # return the feature dataframe\n",
    "    return feature_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
